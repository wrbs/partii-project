function u0:0(r64, r64) -> r64, i64 system_v {
    gv0 = symbol u1:0
    gv1 = symbol u1:1
    gv2 = symbol u1:2
    sig0 = (r64, r64, r64) -> r64, i64 system_v
    sig1 = (i64, i64, i64) system_v
    sig2 = (r64, i64) -> r64, i64 system_v
    fn0 = u0:1 sig0
    fn1 = %Memmove sig1
    fn2 = u0:2 sig2

block0(v0: r64, v1: r64):
    v4 = symbol_value.i64 gv0
    v5 = load.r64 notrap aligned v1+8
    v23 -> v5
    v6 = load.r64 notrap aligned v1
    v24 -> v6
    v7 = symbol_value.i64 gv1
    v25 -> v7
    v28 -> v7
    v8 = load.i64 notrap aligned v7
    v9 = load.r64 notrap aligned v8+360
    v10 = load.r64 notrap aligned v9+120
    v11, v12 = call fn0(v10, v6, v5)
    brz v12, block2(v11)
    jump block3

block3:
    v14 = load.i64 notrap aligned v4+152
    v35 = iconst.i64 -1
    v15 = iadd.i64 v12, v35
    v36 = iconst.i32 3
    v16 = ishl.i64 v12, v36
    v17 = iadd v14, v16
    v37 = iconst.i64 -24
    v18 = iadd v14, v37
    call fn1(v18, v14, v16)
    v19 = iconst.i64 1
    v20 = symbol_value.i64 gv2
    store notrap aligned v19, v17-8
    store notrap aligned v19, v17-16
    store notrap aligned v20, v17-24
    store notrap aligned v17, v4+152
    v21, v22 = call fn2(v11, v15)
    jump block2(v21)

block2(v13: r64):
    v26 = load.i64 notrap aligned v7
    v27 = load.r64 notrap aligned v26+2528
    v29 = load.i64 notrap aligned v7
    v30 = load.r64 notrap aligned v29+2520
    v31 = load.r64 notrap aligned v30+8
    v32 = load.i64 notrap aligned v4+152
    v38 = iconst.i64 -32
    v33 = iadd v32, v38
    store notrap aligned v27, v33
    store.r64 notrap aligned v6, v33+8
    store.r64 notrap aligned v5, v33+16
    store notrap aligned v13, v33+24
    store notrap aligned v33, v4+152
    v34 = iconst.i64 4
    jump block1

block1:
    return v31, v34
}
