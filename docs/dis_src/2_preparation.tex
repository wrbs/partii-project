% !TeX root = ../dissertation.tex
\chapter{Preparation}

The project has its theoretical foundation in the Compiler Construction and Optimising Compilers
course. Implementation of JIT compilation and garbage collection are beyond what is covered in the
Tripos.

OCaml has no specification (formal or otherwise) and a large part of project preparation was
familiarising myself with the low-level details of OCaml and gaining familiarity with the existing
runtime's source code.

\section{Technology choices}

The Rust programming language is not covered in the Tripos but I am already very familiar with
it from personal projects. Its combination of ML-like static type system, excellent performance
comparable to C and excellent FFI with C made it a good fit for this project. The language's
support
for algebraic data types and pattern matching (with the compiler verifying all cases are matched)
were particularly useful for the project.

Although many beginners to the language find themselves fighting the borrow checker I had used Rust
enough that I ran into no such problems. The language supports building high-level abstractions
while retaining performance.

x86\_64 assembly was new to me but did not prove too challenging to learn given my experience with
a variety of other architectures.

\section{The OCaml bytecode interpreter}

OCaml bytecode is interpreted by a stack-based abstract machine optimised for patterns in
functional programming. The OCaml runtime includes a generational, tracing and precise
stop-the-world garbage collector.

\subsection{Data representation}

OCaml has a uniform data representation for its \emph{values}. Values are 64\footnote{on 32-bit
    systems all of this is different, but this project only targets \texttt{x86\_64}} bits long.
Pointers to heap-allocated values are stored directly - but due to alignment they are guaranteed to
have a 0 in
the LSB. Integers are 63 bits long with the unused LSB storing a value of 1.

Every heap allocated value has a 64-bit header containing the number of words stored (called
the \texttt{wosize}) and a \texttt{u8} tag. Most tag values correspond to a block which can be
thought of as a tagged tuple containing \texttt{wosize} fields. Each field is treated as an OCaml
value by the garbage collector.

There are special tag values and cases in the garbage collector for things like floating point
numbers (which due to the uniform representation must be stored boxed on the heap),
closures, objects and \texttt{f64}/\texttt{u8} arrays.

\subsection{Registers}

The OCaml abstract machine uses five registers:

\begin{itemize}
    \item \texttt{sp} is a stack pointer for the OCaml stack which is used extensively
          in the bytecode.
    \item \texttt{accu} is an accumulator register used for the return values of functions
          and primitives as well as for most arithmetic operations.
    \item \texttt{env} holds a pointer to the current closure (an OCaml block) which is used for
          referencing closure variables (stored as fields in the block)
    \item \texttt{extra\_args} is used to mark the number of arguments passed on the stack
    \item \texttt{pc} contains the pointer to the bytecode instruction to be interpreted next
\end{itemize}

In addition to these registers there is the \texttt{Caml\_state} struct whose fields can be
considered as another 30 or so registers. They are used by the interpreter mainly for supporting
the garbage collector, exceptions and growing and reallocating the OCaml stack.

\subsection{Function calling model}

The interpreter traces its lineage from the ZINC Abstract Machine (ZAM) \cite{zinc} through various
iterations of the Caml system. The most significant feature of these abstract machines for this
project was the model used for calling functions and dealing with the functional language concept
of partial application.

\subsubsection{Eval-apply vs push-enter}

In a 2005 talk at the KAZAM workshop \cite{xavtalk}, Xavier Leroy describes the concept of a
distinction between the \emph{eval-apply} and \emph{push-enter} models. These models were
originally due to Simon Peyton Jones \cite{jones}\cite{marlow-jones}. This distinction has to do
with how functions deal with taking
multiple arguments and is particularly relevant to functions taking curried arguments.

In the eval-apply model (followed by most imperative programming languages like C, Java or Python
or
the OCaml native code backend) a function has a set number of arguments it takes. If a caller
provides more or less than the required arguments (partial application or calling a function
returned by a function) the caller must contain the code to handle these cases.

By contrast, in the push-enter model the callee must support any number of arguments passed to it.
This is the method used by the OCaml bytecode compiler and interpreter. The mechanism for doing
this is the combination of the stack and \texttt{extra\_args} register. The way the system works is
somewhat intricate and becomes relevant in this project in section \ref{dyn-recomp} where it is
explained in more detail.

\subsection{Garbage collector}

OCaml is a garbage collected language - memory is managed by the runtime and released once it is
no longer reachable from any other live object by the garbage collector (GC).

In the typical classification of tracing garbage collectors, the OCaml garbage collector is a
precise and generational stop-the-world garbage collector.

\subsubsection{Generational}

As OCaml is a functional language, short-lived immutable values are created and dropped very
frequently. For this reason OCaml uses a generational garbage collector: there is a minor heap and
a major heap. Allocations are done by pointer bump in the minor heap
until it becomes full. At that point the runtime branches into the garbage collector which compacts
anything alive in the minor heap.

Values that survie for some period of time are moved to the major heap which is collected much less
frequently.

OCaml requires a write barrier for every assignment to a field.

\subsubsection{Precise}

A precise tracing garbage collector can correctly identify every reference to an object and
determine exactly which values are pointers and which are other memory. In the OCaml interpreter
this is accomplished by storing all values on the OCaml stack and using the uniform data
representation to distinguish integers from pointers.

My initial compiler reuses the OCaml stack. The optimising compiler requires more sophisticated
handling, described in section \ref{gc-support}.

\subsubsection{Safepoints}

A useful abstraction in the implementation of code interacting with a GC is that of the safepoint.
This is a point in the program (usually a function call) where pointers might end up relocated.

For OCaml this can happen:

\begin{itemize}
    \item when the minor heap is full during an allocation and the allocation routine branches
          into
          the
          GC
    \item when a C primitive is called	(which may itself allocate memory and trigger the
          GC)
    \item when responding to signal handlers
\end{itemize}

As the garbage collector relocates objects, it is important the runtime can find all GC roots
(pointers to heap-allocated values). For the interpreter, all roots are stored on the stack. For
this reason the interpreter spills the accu and env to the stack at every safepoint.

\subsection{Instruction format} \label{ocaml-bytecode-format}

Opcodes and arguments are all stored as \texttt{i32} values. There are 149 distinct opcodes taking
various
different numbers of arguments.

However many opcodes are just the composition of a few simpler opcodes to save space in the format.
For example \texttt{PUSHGETGLOBALFIELD(x, y)} can be expanded to \texttt{PUSH,
    GETGLOBAL(x), GETFIELD(y)} or \texttt{ACC2} (only opcode) is the same as \texttt{ACC(2)}
(\texttt{ACC} opcode with the 2 as the operand).

\section{Compiler concepts}

A \emph{basic block} represents a sequence of instructions with only one entry and exit - all
instructions
in the block will be executed in order without any control flow.

These blocks are typically combined to make a data structure called a \emph{control flow graph}.
Each vertex represents a basic block and the (directed) edges represent the potential flow of
control between basic blocks.

\subsection{SSA form}

A useful intermediate form used in compilers is single static assignment (SSA) form. If a program
is in this form,
every variable is assigned to only once and only binds a single immutable value.

This form is very useful to compiler writers as it can simplify the presentation and implementation
of many optimisations.

In order to allow for conditional branching in the program, the form needs some method to mark
choosing between values to use depending on the path taken through the program. The typical
method, as covered in the Optimising Compilers course, uses special Phi instructions to allow for
this type
of branching. The successor blocks have a node which marks the value that should be used in the
case
of each entry edge into the function.

\begin{figure}
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \begin{tikzpicture}[auto,
                node distance = 12mm,
                start chain = going below,
                box/.style = {draw, rounded corners, blur shadow, fill=white, on chain,
                        align=center}
            ]

            \node[box] (b1) {$\bm{b_1}$ \\ $v_1 \leftarrow x + y$ \\ $(v_1 >= 0)$? $b_2$ : $b_3$};
            \node[box] (b2) [below left=of b1,xshift=15mm]{$\bm{b_2}$ \\ $v_2 \leftarrow 5$ \\
                jump
                $b_4$};
            \node[box] (b3) [below right=of b1,xshift=-15mm] {$\bm{b_3}$ \\ $v_3 \leftarrow 7$ \\
                jump $b_4$};
            \node[box] (b4) [below=35mm of b1] {$\bm{b_4}$ \\ $v_4 \leftarrow \Phi
                    (b_2 \mapsto
                    v_2, b_3
                    \mapsto
                    v_3)$\\print($v_4$)};

            \begin{scope}[rounded corners, -latex]
                \path (b1) edge (b2) (b2) edge (b4);
                \path (b1) edge (b3) (b3) edge (b4);
            \end{scope}
        \end{tikzpicture}
        \caption{Phi nodes}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \begin{tikzpicture}[auto,
                node distance = 12mm,
                start chain = going below,
                box/.style = {draw, rounded corners, blur shadow, fill=white, on chain,
                        align=center}
            ]

            \node[box] (b1) {$\bm{b_1}$ \\ $v_1 \leftarrow x + y$ \\ $(v_1 >= 0)$? $b_2$ :
                $b_3$};
            \node[box] (b2) [below left = of b1,xshift=20mm]{$\bm{b_2}$ \\ $v_2 \leftarrow 5$ \\
                jump
                $b_4(v_2)$};
            \node[box] (b3) [below right = of b1,xshift=-20mm] {$\bm{b_3}$ \\ $v_3 \leftarrow 7$ \\
                jump
                $b_4(v_3)$};
            \node[box] (b4) [below = 35mm of b1] {$\bm{b_4(v_4)}$ \\\\ print($v_4$)};

            \begin{scope}[rounded corners, -latex]
                \path (b1) edge (b2) (b2) edge (b4);
                \path (b1) edge (b3) (b3) edge (b4);
            \end{scope}
        \end{tikzpicture}

        \caption{Block parameters}

    \end{subfigure}
    \caption{Comparison of phi nodes and block parameters for SSA forms}
    \label{fig:phi-bp}
\end{figure}

Cranelift, and by extension this project, uses an alternative formulation known as block
parameters.
Here the blocks appear as if they are functions taking arguments and the values to use for the
arguments are provided by the predecessor block. This is a newer, slightly clearer formulation that
avoids having special case Phi node meta-instructions.

The difference between Phi nodes and block parameters is shown in Figure \ref{fig:phi-bp}.
\section{x86\_64}

\begin{table}[h]
    \centering

    \begin{tabular}{ll}\toprule
        Argument registers     & rdi, rsi, rdx, rcx, r8, r9                \\
        Return registers       & rax, rdx                                  \\
        Stack alignment        & 16-byte at call                           \\
        Callee-saved registers & rax, rdi, rsi, rdx, rcx, r8, r9, r10, r11 \\
        Caller-saved registers & rbx, rsp, rbp, r12, r13, r14, r15         \\
        \bottomrule
    \end{tabular}

    \caption{Summary of the System V x86\_64 calling convention}
    \label{table:systemv}

\end{table}

x86\_64 is a large CISC (complex instruction set computer) architecture descending from the Intel
8086 processor.
It has 16 general purpose 64-bit registers: \texttt{r[abcd]x, rdi, rsi, rsp, rbp, r8-r15}.
This
project targets Linux only which uses the System V
calling convention. Assembly listings in this project use Intel
    [\texttt{mov eax, 1}] rather than AT\&T [\texttt{movl
    \$1, \%eax}] syntax.

A summary of the System V calling convention is given in Table \ref{table:systemv} --- the relevant
details for this dissertation are
that up to six 64-bit arguments can be passed to functions in registers, that up to two values may
be returned by a
function
and the large number of caller-saved registers.
\section{Dependencies used}

I use a lot of open source libraries/crates (as is usual to do in Rust). Some dependencies needed
to be tweaked slightly and where that is the case they are vendored in to the source tree. Other
dependencies are included directly using \texttt{Cargo.toml}.

Rust crate dependencies are under various licenses but all can be linked with my work in the main
project which to be compatible with OCaml is LGPL 2.1 only with the OCaml linking exception. Most
of
these dependencies are transitive. A full list is given in Appendix \ref{appendix-license}.

\subsection{Dynasm}

\textbf{TODO - Pull a bit out of impl for this}

\subsection{Cranelift}

The second compiler uses the \texttt{cranelift} \cite{cranelift} library which is a low-level
retargetable code generator with an emphasis on use in JITs\footnote{It's largest use is for
    JIT-compiling webassembly as part of \texttt{wasmtime} and Firefox}. It is somewhat similar to
LLVM
but lower level and much simpler. It cannot perform many optimisations LLVM can but has
significantly lower compilation time.

\subsection{Sandmark}

I based my benchmark suite from the excellent Sandmark project by OCaml Labs at Cambridge. The
project consists of benchmark sources, build scripts (using \texttt{dune}), a benchmark runner,
compiler definitions (using \texttt{opam}) and a complicated \texttt{Makefile} to tie it all
together.  I made some larger changes to the tool to support bytecode benchmarks (the project
initially only benchmarked the native code compiler) but was able to reuse most of the machinery.

\section{Starting point}

Before the formal project start I forked the OCaml compiler (version 4.11.1) and implemented a
proof of concept bidirectional FFI between C and Rust. This was done to mitigate the risk of not
being able to successfully link the OCaml runtime with Rust and helped build some familiarity with
OCaml's existing runtime.

I also wrote a parser from the bytecode into Rust data types and a simple disassembler that uses
it. This allowed me to explore the undocumented bytecode format and begin to understand how higher
level OCaml constructs map to bytecode concepts.

Both components were almost completely rewritten during the progress of the project.

\section{Development methodology}

This project was developed in an iterative manner most closely aligned with the 'Agile'
methodology.  A key focus was on working code that could be tested at every stage of the project.
Support for opcodes were added incrementally as needed.

Components were initially developed as simply as possible with limited optimisation. Performance
profiles were used to determine things to optimise. Some components were rewritten once they were
determined to be too inflexible or lacking in performance --- the Rust type system was very
effective
at ensuring all places that needed to be changed in refactoring were updated.

\subsection{Testing}

Automated testing was essential to the development of this project. A variety of testing strategies
were
employed: unit tests of easily isolatable components, comparison testing of input/output behaviour
for phases of the compiler and whole system tests in the OCaml compiler. The most useful type of
test
was the trace comparison test against the behaviour of the existing interpreter.

As OCaml bytecode has no specification outside of the behaviour of the interpreter, it makes sense
to test
directly against this behaviour. The VM state was printed at every instruction and automated
tooling would
compare the JIT's trace against the interpreter's one. More details are given in Section
\label{trace-comparison}.  \ref{trace-comparison} to ensure identical execution behaviour to the

To develop larger components like the optimising compiler before enough code was written to allow
running the entire compiler, I made use of `expect tests'. These consist of tests that compare a
string representation of the compiler output against a reference string included in the source. The
expect test runner will show the diff on failure and has support for `promoting' the new version
replacing the reference string with the new output. In addition to effectively detecting unforeseen
regressions these tests serve as good self-contained test cases of components.

\subsection{Tools}

All project code was stored in a single Git repository with all of the different subcomponents
version controlled together (typically known as a `monorepo'). GitHub was used as a reliable remote

storage for backup. Experimental ideas were developed on branches to allow evaluation of different
strategies and I used self-reviewed pull requests to review changes.

Most development of the project mostly occurred in periods of a few days/weeks where nearly
all of my working time was dedicated to the project. However, I made sure to spend at least 1 day
on the project per week even when focussing on other commitments. This helped retain the project
knowledge between the more intense working sessions.

The OCaml test suite was used towards the later stages of completeness and failures were added as
cases to my test suite as trace comparisons. This helped isolate and fix rare bugs in uncommon
code.

The \texttt{cargo} build system was used for all Rust code. Linking in to the OCaml runtime
was done by modifying OCaml's rather messy \texttt{autoconf} \& handwritten \texttt{Makefile} build
system. Smaller automation was done with bash scripts with more complicated tools written in Rust.

All of these components are tied together with a toplevel Makefile. The project can be used as a
custom
\texttt{opam} switch making it easy to test the system with the entire ecosystem of OCaml
dependencies.
This is used by the Sandmark benchmark suite which itself uses \texttt{dune} to build and run
benchmark programs.

Data analysis is done using the standard Python stack (pandas, matplotlib, jupyter, etc.). A frozen
list of Python dependencies is at \texttt{benchmarks/requirements.txt}.

I used a combination of neovim, Visual Studio Code (with \texttt{rust-analyzer}) and CLion as text
editors/IDEs for the project. All documentation and research was done solely using the Internet.
Clippy was used for linting Rust code and all code was autoformatted using \texttt{rustfmt}.

\subsection{Resources}

I used a desktop I built in 2020 (Intel 10700K CPU, 32GB RAM on fast SSD) to develop the project.
The machine triple-boots Windows, Arch Linux and macOS but by the end the simplifying assumption
was made only to support Linux (meaning the project was developed both with WSL or on Arch). The
project used to work perfectly on \texttt{macOS} and would likely only take minor modifications to
restore support. However, with Apple moving away from x86\_64 I decided to stop testing it and
since
then one part of the project uses a Linux-only syscall. It would not require too much work to
restore support to macOS.

The project also works on my significantly older and less powerful laptop but compilation is
slower.
The repository contains everything needed to bootstrap the project and in case of catastrophic
system failure I would only lose anything not pushed to GitHub.

Separately of GitHub, I have automatic synchronisation set up to a server I rent. This server
itself
uploads encrypted, incremental backups to OneDrive.
