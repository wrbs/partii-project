% !TeX root = ../dissertation.tex
\chapter{Preparation}

The OCaml bytecode interpreter has no collected specification or documentation. Although aspects of
the runtime system are explained in the OCaml documentation from the perspective of implementing a
foreign-function interface (FFI) to C, most necessary implementation details can only be found
through study of the compiler's source code. For this reason, I had performed a significant amount
of work prior to the formal project start, investigating whether the project was even feasible.
Investigation into OCaml's low-level details continued to be required throughout the project.

After this work was complete, I was in a position to write and then immediately start on the
project
plan given in the proposal. This plan set out the steps required to build a simpler version of the
initial compiler. Work proceeded according to the optimistic schedule leaving time to build the
significantly more sophisticated optimising compiler.

\section{Relation to the Tripos}

This project built on concepts from many areas of the Tripos. The most directly
relevant courses are Part IB Compiler Construction and Part II Optimising Compilers, but the
project required
reading beyond the scope of either.

This project also used a large amount of low-level knowledge of the x86\_64 architecture and Linux,
as covered in Programming in C and C++, Computer Design and Operating Systems.

OCaml was not covered in the Tripos beyond its use as the implementation language for Compiler
Construction but shares many similarities with Standard ML, as was covered in Foundations of
Computer
Science\footnote{When I took the course --- it now uses OCaml.}. Although this project operated at
a
much
lower level of abstraction, knowledge of the
source language was crucial for understanding what a sequence of bytecode instructions was doing.

\section{Technology choices}

The Rust programming language is not covered in the Tripos but I was already very familiar with
it from personal projects. The language's support for algebraic data types and pattern matching
(with the compiler verifying all cases are matched) was particularly useful for the project.

x86\_64 assembly was new to me but did not prove too challenging to learn given my experience with
a variety of other architectures.

\section{The OCaml bytecode interpreter}

OCaml bytecode is interpreted by a stack-based abstract machine, optimised for patterns in
functional programming.

\subsection{Data representation}

OCaml has a uniform data representation for its \emph{values}. Values are 64 bits long.
Pointers to heap-allocated values are stored directly --- but due to alignment they are guaranteed
to
have a 0 in
the LSB. Integers are 63 bits long with the unused LSB storing a value of 1.

Every heap-allocated value has a 64-bit header containing the number of words stored (called
the \texttt{wosize}) and a \texttt{u8} tag. Most tag values correspond to a block, which can be
thought of as a tagged tuple containing \texttt{wosize} fields. Each field is treated as an OCaml
value by the garbage collector.

There are special tag values and cases in the garbage collector for elements like floating point
numbers (which, due to the uniform representation, must be stored boxed on the heap),
closures, objects and \texttt{f64}/\texttt{u8} arrays.

\subsection{Registers}

The OCaml abstract machine uses five registers:

\begin{itemize}
    \item \texttt{sp} is a stack pointer for the OCaml stack, which is used extensively
          in the bytecode.
    \item \texttt{accu} is an accumulator register used for the return values of functions
          and primitives as well as for most arithmetic operations.
    \item \texttt{env} holds a pointer to the current closure (an OCaml block), which is used for
          referencing closure variables (stored as fields in the block).
    \item \texttt{extra\_args} is used to mark the number of arguments passed on the stack.
    \item \texttt{pc} contains the pointer to the bytecode instruction to be interpreted next.
\end{itemize}

In addition to these registers there is the \texttt{Caml\_state} struct whose fields can be
considered as another 30 or so registers. They are used by the interpreter mainly for supporting
the garbage collector, exceptions and growing and reallocating the OCaml stack.

\subsection{Function-calling model}

The interpreter descends from the ZINC Abstract Machine (ZAM) \cite{zinc} through various
iterations of the Caml system. The most significant feature of these abstract machines for this
project is the model used for calling functions and dealing with the functional-language concept
of partial application.

\subsubsection{Eval-apply vs push-enter}

In a 2005 talk at the KAZAM workshop \cite{xavtalk}, the creator of OCaml, Xavier Leroy, describes
the concept of a distinction between the \emph{eval-apply} and \emph{push-enter} model.
These models were originally due to Simon Peyton Jones \cite{jones}\cite{marlow-jones}. This
distinction has to do with how functions deal with taking multiple arguments, and is particularly
relevant to functions taking curried arguments.

In the eval-apply model --- followed by most imperative programming languages and the OCaml
native-code backend --- a function has a set number of arguments it takes. If a caller provides
fewer or more than the required arguments (partial application or calling a function returned by a
function) the caller must contain the code to handle these cases.

By contrast, in the push-enter model the callee must support any number of arguments passed to it.
This is the method used by the OCaml bytecode compiler and interpreter. The mechanism for doing
this is somewhat intricate and becomes relevant in this project in section \ref{dyn-recomp}, where
it is explained in more detail.

\subsection{Garbage collection}

OCaml is a garbage-collected language --- memory is managed by the runtime and released once it is
no longer reachable from any other live object by the garbage collector (GC). The OCaml garbage
collector is precise --- it needs to be able to identify exactly the set of values at any point in
the program. This requirement was a significant source of complication, especially towards the end
of the project.

% \subsubsection{Precise}
% 
% A precise tracing garbage collector can correctly identify every reference to an object and
% determine exactly which values are pointers and which are other memory. In the OCaml interpreter
% this is accomplished by storing all values on the OCaml stack and using the uniform data
% representation to distinguish integers from pointers. This
%
% \subsubsection{Safepoints}
% 
% A useful abstraction in the implementation of code interacting with a GC is that of the safepoint.
% This is a point in the program (usually a function call) where pointers might end up relocated.
% 
% For OCaml this can happen:
% 
% \begin{itemize}
%     \item when the minor heap is full during an allocation and the allocation routine branches
%           into the GC
%     \item when a C primitive is called	(which may itself allocate memory and trigger the
%           GC)
%     \item when responding to signal handlers
% \end{itemize}
% 
% As the garbage collector relocates objects, it is important the runtime can find all GC roots
% (pointers to heap-allocated values). The key invariant is:
% 
% \begin{framed}
%     At every safepoint, the garbage collector should be able to identify every pointer to an OCaml
%     heap-allocated value. It must be assumed that these pointers have changed.
% \end{framed}
% 
% For this reason the interpreter spills the contents of the accu and env to the stack at every
% safepoint and restores them afterwards.

\section{Compiler concepts}

A \emph{basic block} represents a sequence of instructions with only one entry and exit --- all
instructions
in the block will be executed in order.

These blocks are typically combined to make a data structure called a \emph{control flow graph}.
Each vertex represents a basic block and the directed edges represent the potential flow of
control between basic blocks. Figure \ref{fig:phi-bp} shows example control flow graphs.

\begin{figure}[h]
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \begin{tikzpicture}[auto,
                node distance = 12mm,
                start chain = going below,
                box/.style = {draw, rounded corners, blur shadow, fill=white, on chain,
                        align=center}
            ]

            \node[box] (b1) {$\bm{b_1:}$ \\ $v_1 \leftarrow x + y$ \\ $(v_1 \ge 0)$? $b_2$ :
                $b_3$};
            \node[box] (b2) [below left=of b1,xshift=15mm]{$\bm{b_2:}$ \\ $v_2 \leftarrow 5$ \\
                jump
                $b_4$};
            \node[box] (b3) [below right=of b1,xshift=-15mm] {$\bm{b_3:}$ \\ $v_3 \leftarrow 7$ \\
                jump $b_4$};
            \node[box] (b4) [below=35mm of b1] {$\bm{b_4:}$ \\ {\color{Bittersweet}$v_4 \leftarrow
                \Phi
                (b_2 \mapsto
                v_2, b_3
                \mapsto
                v_3)$}\\print($v_4$)};

            \begin{scope}[rounded corners, -latex]
                \path (b1) edge (b2) (b2) edge (b4);
                \path (b1) edge (b3) (b3) edge (b4);
            \end{scope}
        \end{tikzpicture}
        \caption{Phi nodes}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \begin{tikzpicture}[auto,
                node distance = 12mm,
                start chain = going below,
                box/.style = {draw, rounded corners, blur shadow, fill=white, on chain,
                        align=center}
            ]

            \node[box] (b1) {$\bm{b_1:}$ \\ $v_1 \leftarrow x + y$ \\ $(v_1 \ge 0)$? $b_2$ :
                $b_3$};
            \node[box] (b2) [below left = of b1,xshift=20mm]{$\bm{b_2:}$ \\ $v_2 \leftarrow 5$ \\
            jump
            $b_4{\color{Bittersweet}(v_2)}$};
            \node[box] (b3) [below right = of b1,xshift=-20mm] {$\bm{b_3:}$ \\ $v_3 \leftarrow 7$
            \\
            jump
            $b_4{\color{Bittersweet}(v_3)}$};
            \node[box] (b4) [below = 35mm of b1] {$\bm{b_4{\color{Bittersweet}(v_4)}:}$ \\\\
            print($v_4$)};

            \begin{scope}[rounded corners, -latex]
                \path (b1) edge (b2) (b2) edge (b4);
                \path (b1) edge (b3) (b3) edge (b4);
            \end{scope}
        \end{tikzpicture}

        \caption{Block parameters}

    \end{subfigure}
    \caption{Comparison of phi nodes and block parameters for SSA forms.}
    \label{fig:phi-bp}
\end{figure}

\subsection{SSA form} \label{ssa}

A useful intermediate form used in compilers is static single assignment (SSA) form. If a program
is in this form,
every variable is assigned to only once and only binds a single immutable value.

This form is very useful to compiler writers as it can simplify the presentation and implementation
of many optimisations.

In order to support conditional branching in the program, the form needs some method to mark
choosing between values to use depending on the path taken through the program. The typical
method\footnote{As covered in the Optimising Compilers course.} uses special phi node in the
successor block to mark these cases.

This project uses an alternative method for this: \emph{block parameters}.
Here the blocks appear as if they are functions taking arguments and the values to use for the
arguments are provided by the predecessor block in the branch instructions --- see Figure
\ref{fig:phi-bp} for an example. This is a newer and slightly cleaner formulation --- but in
typical uses
the two representations are isomorphic.\footnote{In fact, block parameters are slightly more
    general as different values can be passed from the same predecessor block --- such as if both
    branches of a conditional branch entered the same block with a different value. This is rare in
    practice.}

\section{x86\_64}

\begin{table}[h]
    \centering

    \begin{tabular}{ll}\toprule
        Argument registers     & rdi, rsi, rdx, rcx, r8, r9                \\
        Return registers       & rax, rdx                                  \\
        Stack alignment        & 16-byte at call                           \\
        Callee-saved registers & rax, rdi, rsi, rdx, rcx, r8, r9, r10, r11 \\
        Caller-saved registers & rbx, rsp, rbp, r12, r13, r14, r15         \\
        \bottomrule
    \end{tabular}

    \caption{Summary of the System V x86\_64 calling convention}
    \label{table:systemv}

\end{table}

x86\_64 is a large CISC (complex instruction-set computer) architecture descending from the Intel
8086 processor.
Assembly listings in this project use Intel
    [\texttt{mov eax, 1}] rather than AT\&T [\texttt{movl
    \$1, \%eax}] syntax.

This project targets Linux only, which uses the System V calling convention.  A summary of the
System V calling convention is given in Table \ref{table:systemv} --- the relevant details for this
dissertation are that up to six 64-bit arguments can be passed to functions in registers, that up
to two values may be returned by a function and that there are a larger large number of
caller-saved
registers.

\section{Requirements analysis}

The project plan set out ambitious success criteria.

\begin{enumerate}
    \item There is a JIT compiler implemented into the existing OCaml source
          replacing the interpreter with all functionality but debugging
          and introspection.
    \item There is a comprehensive and automated suite of benchmarks built,
          comparing its performance to other alternatives.
    \item It performs favourably to the interpreter.
\end{enumerate}

To avoid scope creep, I decided on some additional requirements:

\begin{enumerate}
    \item The only target is x86\_64 Linux; make no attempt at portability.
    \item For the initial compiler, correctness $>$ completeness $>$ performance.
\end{enumerate}

The last of these requirements was changed during the implemenation of the second compiler:
performance was considered more important than completeness.

\section{Dependencies and licences}

As is common within the Rust ecosystem, I made use of third-party dependencies. There are many
transitive dependencies that I have not explicitly chosen linked in to the final binary. My project
integrates heavily with the OCaml runtime, so is licensed under LGPL 2.1 too.

Rust crate dependencies are under various licences. A full list is given in Appendix
\ref{appendix-licence}; all licences are compatible with LGPL 2.1.

Some of my Rust dependencies needed to be tweaked slightly; their source is
vendored in to the project tree. My tweaks there are licenced under the licences of the modified
crates.

\subsection{Dynasm-rs}

The initial compiler uses \texttt{dynasm-rs} \cite{dynasmrs}, which works as a Rust macro, turning
assembly instructions into pre-assembled code, and calls into a simple runtime for relocation. Its
design is based on that of \texttt{DynASM} used in the \texttt{LuaJIT} project \cite{dynasm}.

\subsection{Cranelift}

The second compiler is designed around the cranelift \cite{cranelift}, library which is a
low-level retargetable code generator with an emphasis on use in JITs\footnote{It's largest use is
    for JIT-compiling webassembly as part of \texttt{wasmtime} and Firefox}. It is written in Rust
and so benefits from an API designed for the language.

\subsection{Sandmark}

I based my benchmark suite on the excellent Sandmark project by OCaml Labs at Cambridge. The
project consists of benchmark sources, build scripts (using \texttt{dune}), a benchmark runner,
compiler definitions (using \texttt{opam}) and a complicated \texttt{Makefile} to tie it all
together.  I made some larger changes to the tool to support bytecode benchmarks (the project
initially only benchmarked the native-code compiler) but was able to reuse most of the machinery.

Sandmark is licensed under CC0 (it is effectively public domain).

\section{Starting point}

The starting point of this project was the OCaml compiler 4.11.1, Sandmark from the commit hash
starting
\texttt{09862492}\footnote{\url{https://github.com/ocaml-bench/sandmark/commit/09862492680c296fd659
        ceef9b34035ab97f7fe6}}, and the source of any other vendored dependencies (identifiable by
being
included in under directories called \texttt{vendor}). The patches made to vendored dependencies
are
so small that it is best to consider them not my work for the purpose of this assessment.

Some prototyping performed was done before project start.  The first aspect of this work was
modifying
the Makefiles of the OCaml compiler to link with a
Rust-produced static library and ensure they could both call functions from the other. The other
component was a simple disassembler for OCaml bytecode to gain familiarity with the format. Both
aspects of this work were extended during the project and the current remnants of that work are
minimal --- limited to some of the changes to the OCaml Makefiles and the list of opcodes in
\texttt{src/rust/ocaml-jit-shared/src/opcodes.rs}).  The original disassembler was replaced with a
more sophisticated one\footnote{Not covered in this document due to space constraints but it
    can be found in the \texttt{src/rust/ocaml-jit-tools} directory.}.

\section{Development methodology} \label{dev-methodology}

This project was developed in an iterative manner most closely aligned with the Agile
methodology. Much of the Agile manifesto is based around customer and team collaboration which is
not as relevant for this largely solitary project. However, many of the principles are still useful
for single-developer projects. There are many practices which call themselves Agile. I mean:

\begin{itemize}
    \item Prioritise delivering working software as frequently as possible.
    \item Work on small achievable tasks in a tight development loop encompassing all stages of the
          development lifecycle (planning, implementation and testing).
    \item Favour responding to change over following a plan; use tests and static type systems to
          make refactoring existing code an inexpensive and safe process.
    \item Reflect on progress and process at regular intervals and adjust behaviour accordingly.
\end{itemize}

The advantage of the methodology for my project is it allows for changing requirements. Changing
requirements are  usually something to be avoided but this is not always possible.  Although the
overall project goals did not change, the project interacts at a low level with many complicated
aspects of the existing OCaml runtime. I did not have the knowledge to create a detailed plan for
dealing with these systems. Agile embraces changing requirements making it an effective choice for
this project.

Working without a detailed plan introduces a risk of falling behind without realising or wasting
time on over-engineering. To mitigate this problem I used two strategies:

\begin{enumerate}
    \item Set deadlines for each major component of the project with my supervisor, leading to
          accountability and external motivation.
    \item Agree the goals and tasks for each week in a meeting with my project supervisor,
          allowing us to keep track of the progress towards the major deadlines.
\end{enumerate}

Development followed an rapid iterative cycle of experimentation, short plans, implementation and
testing, all directed to solving a clearly defined goal. I found this a particularly effective
methodology for this project and my style of working.

\subsection{Testing}

Automated testing was essential to the development of this project. I used unit tests where it was
appropriate for small functions or modules. However, by far the most useful class of test was
automated trace comparison against the exact behaviour of the existing interpreter.

As OCaml bytecode has no specification outside the behaviour of the interpreter it can be hard
to know exactly what the behaviour should be in all cases solely from the interpreter source code.
To get around this I used automated comparison testing: the VM state was printed at every
instruction and automated
tooling would compare the JIT's trace against the interpreter's. More details are given in
Section \ref{trace-comparison}.

To develop larger components, like the optimising compiler, before enough code was written to allow
running the entire compiler, I made use of `expect tests'. These consist of tests that compare
output of components against a reference string included in the source. The expect-test runner will
show the diff on failure and has support for `promoting' the new version, replacing the reference
string with the new output. In addition to detecting regressions these tests serve as good
self-contained examples of components and are the source of most of the output examples in this
document. They are somewhat detached from the exact implementation details while testing components
in isolation which makes them useful from refactoring.

The OCaml test suite was used towards the later stages of completeness and failures were added as
cases to my test suite as trace comparisons. This helped isolate and fix rare bugs in uncommon
code.

% Another technique enabled by Rust's powerful type system was encoding invariants into the type
% system --- commonly referred to as `making illegal states unrepresentable'. I tended to find that if
% code compiled it worked the first time.

\subsection{Tools}

All project code was stored in a single Git repository (a `monorepo' model). GitHub was used to
host the repository online. Experimental ideas were developed on
branches to allow evaluation of different
strategies. I occasionally used self-reviewed pull requests for large changes.

% Most development of the project mostly occurred in periods of a few days/weeks where nearly
% all of my working time was dedicated to the project. However, I made sure to spend at least 1 day
% on the project per week even when focussing on other commitments. This helped retain the project
% knowledge between the more intense working sessions.

The \texttt{cargo} build system was used for all Rust code. Linking in to the OCaml runtime was
achieved by modifying OCaml's build system (autoconf and hand-written Makefiles). Smaller
automation was achieved with bash scripts with more complicated tools written in Rust.	All of
these components are tied together with a toplevel Makefile.

The project can be used as a custom \texttt{opam} switch making it easy to test the system under
different compile-time options with the entire ecosystem of OCaml dependencies --- this is used in
the implementation of the benchmark suite.

Data analysis is preformed using the standard Python stack (pandas, matplotlib, jupyter, etc.). A
frozen
list of Python dependencies is at \texttt{benchmarks/requirements.txt}.

I used a combination of neovim, Visual Studio Code (with \texttt{rust-analyzer}) and CLion as text
editors/IDEs for the project.  Clippy was used for linting Rust code and all code was autoformatted
using \texttt{rustfmt}.

\subsection{Resources}

I used my machine (Intel 10700K CPU, 32GB RAM on fast SSD) to develop the project. The project also
works on my older and less powerful laptop but compilation is slower --- but still usable as a
backup system. The repository contains everything needed to bootstrap the project.