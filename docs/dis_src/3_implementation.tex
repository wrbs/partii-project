% !TeX root = ../dissertation.tex

\newcommand{\codeat}[1]{\normalfont{\small Code at \texttt{#1}}}

\newcommand{\codesection}[2]{
      \section[#1]{#1\\\codeat{#2}}
}
\newcommand{\codesubsection}[2]{
      \subsection[#1]{#1\\\codeat{#2}}
}
\newcommand{\codesubsubsection}[2]{
      \subsubsection[#1]{#1\\\codeat{#2}}
}

\chapter{Implementation}

\begin{figure}[h]
      \centering
      \begin{tikzpicture}[auto,
                  node distance = 12mm,
                  start chain = going below,
                  box/.style = {draw, rounded corners, blur shadow, fill=white, on chain,
                              align=center, minimum height=8mm}
            ]

            \node[box] (ocaml) {OCaml Runtime};
            \node[box] (init) [right=of ocaml] {Initial Compiler};
            \node[box] (code) [below=of init] {Machine Code};
            \node[box] (opt) [right=of init] {Optimised Compiler};
            \node[box] (optcode) [below=of opt] {Optimised Machine Code};

            \begin{scope}[rounded corners, -latex]
                  \path (ocaml) edge (init) (init) edge (code) (code) edge (opt) (opt) edge
                  (optcode);
            \end{scope}
      \end{tikzpicture}

      \caption{Control flow through the compiler}
\end{figure}

I have developed two new JIT compilers from OCaml bytecode to x86\_64 assembly. The first compiler
completely replaces OCaml's bytecode interpreter and
is used for all programs and functions. The second is only used for functions which are called
multiple times at runtime.

\section{Overview}

OCaml bytecode is executed using a program called \texttt{ocamlrun}. Both new compilers are
implemented in a Rust static library which \texttt{ocamlrun} links to. \texttt{ocamlrun} hooks into
this library when it loads bytecode and when it starts interpreting it.

If the JIT is enabled, either by setting an environment variable or if it is enabled by default at
compile time, when the bytecode load hook is run the \textbf{initial compiler} will execute.

The initial compiler parses the bytecode into a stream of instructions and, for each bytecode
instruction, emits assembly with the same semantics to a buffer. After all code has been emitted
(including headers and footers with shared routines) relocations are performed and the buffer is
marked as executable.

When \texttt{ocamlrun} then calls the hook to start interpretation, the library instead jumps to
this assembly code which then performs the same operations as the interpreter - except that every
instruction has effectively been inlined.

When an OCaml closure is applied,\footnote{All non-primitive function calls are translated to
      closure application} a closure execution count is incremented. Once this count passes a
configurable
threshold, the code instead branches in to the \textbf{optimising compiler}.

The optimising compiler operates on the level of a single function. The code generate differs from
the unoptimised machine code in that there is no concept of the OCaml stack and accumulator
register; instead all values are stored in registers. The function's code pointer is updated and
any
future calls (including the originally triggering call) will use the optimised implementation.

\section{Major milestones}

In accordance with the agile methodology the focus throughout all aspects of the project was on
working code that could be tested even if the total wasn't not complete. However it is worth noting
the major milestones where some degree of completeness was achieved (in chronological order).

\begin{enumerate}
      \item Execution of Hello World with the JIT
      \item Passing the OCaml test suite and bootstrapping the compiler
      \item Implementing the benchmark suite
      \item (Extension) Implementing the optimising compiler
\end{enumerate}

\section{Repository overview}

The project was developed using a `monorepo' where everything for the project was contained in one
Git repository. Some components were solely my work (marked \textbf{mine} below). Some components
were `vendored' (marked \textbf{vendor}) - where the source code is included to allow for small
patches to be made. For OCaml and the sandmark benchmark suites the patches were significant enough
for me to consider it a fork containing both my work and the work of others (marked \textbf{fork}).

\dirtree{%
      .1 /.
      .2 benchmarks.
      .3 sandmark\DTcomment{\textbf{fork}: benchmark suite containing source \& tooling}.
      .3 analysis\DTcomment{\textbf{mine}: Jupyter notebooks analysing benchmark results}.
      .2 docs\DTcomment{\textbf{mine}: source of this document}.
      .2 scripts\DTcomment{\textbf{mine}: scripts to run tests and graph basic blocks}.
      .2 src.
      .3 rust.
      .4 ocaml-jit-shared\DTcomment{\textbf{mine}: shared code for the other two crates}.
      .4 ocaml-jit-staticlib\DTcomment{\textbf{mine}: crate which is linked with
            \texttt{ocamlrun}}.
      .4 ocaml-jit-tools\DTcomment{\textbf{mine}: binary crate with standalone tools}.
      .3 ocaml\DTcomment{\textbf{vendor}: contains OCaml compiler 4.11.1}.
      .4 runtime\DTcomment{\textbf{fork}: contains most large patches for the JIT}.
      .3 vendor\DTcomment{\textbf{vendor}: vendored rust dependencies with small patches}.
      .2 test-programs\DTcomment{\textbf{mine, vendor}: OCaml source for test programs}.
      .2 vendor/no-aslr\DTcomment{\textbf{vendor}: A simple wrapper to run a program without ASLR}.
}

All of these components were tied together using a combination of Makefiles, cargo workspaces, OPAM
and dune.

\section{Initial compiler}

The initial compiler was developed before any work on the optimising compiler was performed. In the
process of implementing the initial compiler some modifications had to be made. These modifications
will be covered later in section \ref{dyn-recomp} and this section will mainly cover the initial
state.

\subsection{Overview}

The basic compiler consists of two large components - an instruction parser and an assembly
emitter.

Most of the rest of the files are support for these components and deal with the FFI from and into
the existing runtime.

\subsubsection{Mapping of the OCaml abstract machine}

The initial compiler operates by converting every bytecode instruction into a stream of assembly
instructions with the same semantics. Abstract machine registers are mapped to x86\_64 registers. A
pointer to the \texttt{Caml\_state} struct containing other global interpreter state is also
stored in a register called \texttt{r\_cs} for access to its fields with small code size. The
mapping used is shown in Table \ref{table:regmap}.

\begin{table}[h]
      \centering
      \begin{tabular}{cc}\toprule
            OCaml register          & x86\_64 register \\
            \midrule                                   \\
            \texttt{r\_env}         & \texttt{r12}     \\
            \texttt{r\_accu}        & \texttt{r13}     \\
            \texttt{r\_extra\_args} & \texttt{r14}     \\
            \texttt{r\_sp}          & \texttt{r15}     \\
            \texttt{r\_cs}          & \texttt{rbx}     \\
            \bottomrule
      \end{tabular}

      \caption{Mapping of OCaml to x86\_64 registers}
      \label{table:regmap}
\end{table}

Note that all x86\_64 registers mapped are callee-saved in the System V C calling convention. This
means that no work needs to be performed spilling and restoring them from the C stack to call
C/Rust
primitives (either as part of \texttt{CCall*} instructions or supporting primitives written for the
JIT).

\subsubsection{Mapping of the PC}

Bytecode pointers are mapped to native code pointers and the interpreter's PC is replaced with the
system's instruction pointer. This change is responsible for nearly all of the performance
improvements of the JITed code. The benefits are

\begin{itemize}
      \item Memory accesses are reduced - rather than loading operands and opcodes, they are baked
            in to the machine code
      \item The CPU can predict execution and fill its pipeline further than it can with the use of
            native code pointers
      \item Branch prediction in hot paths becomes more effective as each branch can be predicted
            differently
      \item The CPU can schedule out-of-order execution more effectively across different
            instructions.
\end{itemize}

However, this approach does lead to more contention on the instruction cache - rather than only
containing a single copy of the implementation of each instruction there are many.

\codesubsection{Hooks from OCaml}{src/rust/ocaml-jit-staticlib/src/c\_entrypoints.rs}

The existing OCaml runtime was modified to hook into the JIT at three points:

\begin{enumerate}
      \item After a bytecode section is loaded
      \item Before a bytecode section is released
      \item When the interpreter is called
\end{enumerate}

In most programs there are is only one section loaded (two if callbacks from C to OCaml are
used). However, the interpreter is used in other places such as the toplevel REPL which motivates
the more general multiple section support. Here each section corresponds to a line of code entered
on the toplevel.

Compilation happens after the first hook and an entry is written to a global table containing the
pointer to the buffer. The compiled code takes the form of a single large C function taking no
arguments.

The interpreter call hook calls into the compiled function stored in the section table.  When a
section is released this entry is dropped which frees the memory associated with the section.

\subsection{Instruction parsing}

The first stage in the pipeline is to parse the bytes into a stream of elements of the
\texttt{Instruction} type.

\codesubsubsection{The \texttt{Instruction}
      type}{src/rust/ocaml-jit-shared/src/instructions/types.rs}
\label{instruction-type}

OCaml has 149 opcodes, each of which can take different number of operands. Arguments and operands
are all stored as \texttt{i32} values. As described in section \ref{ocaml-bytecode-format}, some
opcodes
are space-saving aliases for the composition of multiple simpler instructions. To simplify
implementation
I expand these aliases at parse time so generating code only considers the simple primitives.

Rust has support for algebraic sum types called enums (not to be confused with the much more
limited
enums in a language like C or Java). I use this to store the Instruction type with one variant
\footnote{Usually called type constructor in functional programming contexts} per simple
instruction.

Instructions such as integer comparisons and conditional branches have many variations varying only
in the conditional used. This is extracted into its type allowing for a single instruction to
represent
all conditional branches.

Some instructions like branches have label arguments referencing the location of another opcode in
memory. The instruction type is polymorphic over the type of the label to allow for different label
representations depending on use case.

\codesubsubsection{Parser}{src/rust/ocaml-jit-shared/src/instruction/parse.rs}

The parser is implemented using Rust iterators; the parser takes an iterator of \texttt{i32}s and
produces an iterator producing values of type \texttt{Result<Instruction,
      ParseError>}\footnote{Standard rust error handling type defined as \texttt{enum Result<T,E>
            \{Ok(T), Err(E)\}} like OCaml's \texttt{('a, 'b) result}}.

This use of iterators makes the instruction parser consistently performant for large bytecodes -
consumers of the instruction stream take each instruction as they need it and users do not need to
store a vector of parsed instructions in their entirety.

Labels are stored in the OCaml bytecode as indirect offsets relative to the current PC. To simplify
later uses, these are converted to absolute offsets relative to the start of the section.

As single OCaml instructions may parse to more than one \texttt{Instruction}, the type
contains a pseudo-instruction called \texttt{LabelDef} which is emitted at
the start of every OCaml instruction. This also allows a map to be be built from OCaml bytecode
instruction offsets in the translated code.

\codesubsection{Code generation}{src/rust/ocaml-jit-staticlib/src/compiler/emit\_code.rs}

The code generation is the largest aspect of the compiler. It makes use of the \texttt{dynasm-rs}
library. This library manages emitting machine code and relocation information, relocating and
using \texttt{mmap} to mark the code as executable.

The compiler is triggered on the first time a `section' is loaded - for normal programs this is at
startup and for programs using the OCaml toplevel REPL this is after every statement is typed.

The compiler first emits a standard function-header entrypoint which saves callee-saved registers
used by the compiler and aligns the C stack. A longjmp handler is set up for exceptions and then,
for each bytecode instruction, assembly with the same semantics is emitted.

During this process \texttt{dynasm-rs} dynamic labels are used to set up relocations: these
labels are defined before every bytecode instruction and can be referenced by any other
instruction. DynASM translates these at runtime into pc-relative jumps.

After all instructions are done, some shared code used by the instructions is emitted.
\texttt{dynasm-rs} then performs relocations and uses \texttt{mmap} to mark the region of code as
executable.

The overall signature of the assembly produced by this process is a single function taking no
arguments and returning an OCaml value. OCaml closure applications (function calls) do not produce
a stack frame on the C stack - the existing machinery using the OCaml stack is used instead.

\subsubsection{Simple example}

The main code of the compiler itself is contained in a 2,000 line file
(\texttt{src/rust/ocaml-jit-staticlib/src/compiler/emit\_code.rs}). Most of this is taken up by a
Rust large pattern match for each of the bytecode instructions. As a very simple example of what
the code looks like, consider the implementation of the \texttt{Add} instruction. It adds the value
at the top of the OCaml stack to the accumulator and stores the result in the accumulator. Note
that the OCaml integer format means a decrement is required. In the original assembler it is
implemented:

\inputminted{c}{snippets/add.c}

In the compiler the case becomes:

\inputminted{rust}{snippets/add.rs}

This has exactly the same semantics. Note \texttt{r\_accu} and \texttt{r\_sp}
are aliases for \texttt{r13} and \texttt{r15}.	At compile time, the macro component of
\texttt{dynasm-rs} translates it to:

\inputminted{rust}{snippets/add_comp.rs}

Most assembly work is performed at compile time. The byte string above contains the machine code
for
those instructions. This leads to less work being done at compile time, improving the performance
of the compiler.

\subsubsection{Branches}

As an example of the label and relocation support, consider the \texttt{BranchCmp(Comp, i32, L)}
instruction. It
compares the current value of the accumulator to the \texttt{i32} constant using the condition (of
type \texttt{enum Comp {Lt, Gt, ... }}). If the condition is true it branches to the label,
otherwise it passes to the next instruction.

This is implemented:

\inputminted{rust}{snippets/branchcmp.rs}

The macros translate it to:

\inputminted{rust}{snippets/branchcmp_comp.rs}

This shows the assembler work still needing to be done at compile time - relocations.

\subsubsection{Other cases}

Most other cases follow these patterns. Some more involved instructions call into C primitives
I wrote instead (passing and then restoring the registers from the stack).

However, the basic structure of snippets of combined assembly and Rust in a large pattern match
statement remains for all cases.

\subsection{Implementation strategy}

Implementation followed an incremental and highly test-driven strategy over multiple weeks. The
initial focus was on building a system sophisticated enough to run a hello world program
implementing the bare minimum instructions to support this.

I then slowly expanded the complexity of programs, using them to drive the implementation of new
instructions and fixing bugs.

As is mostly inevitable in a project of this complexity there were a significant number of bugs.
Traditional debugging methods like print debugging or using a debugger are difficult to apply in
a JIT context. Some errors resulted in confusing segfaults without any clear indication of what
went
wrong or ability to extract a stack trace (although manually poking registers).

Despite this the actual implementation was remarkably efficient. This is mainly due to the trace
comparison tooling I developed at the very start of the project and continued to expand throughout
the project.

\subsubsection{Trace comparison} \label{trace-comparison}

There is no formal specification for the OCaml interpreter. The semantics of the interpreter are
what
\texttt{interp.c} and other files in the runtime say they are. Given this I decided to build
tooling to test the behaviour of my JIT-compiled code directly against the behaviour of the
interpreter.

In order to do this I added support for tracing after every instruction in both the existing
interpreter and the JIT-compiled code. The log entry contains the instruction executed, state of
all of the OCaml registers and the top five entries on the stack.

There are a few log formats that it can print - for this application I used \texttt{serde} to
serialise and deserialise the trace entries as JSON. The trace comparison program launches the
program: once with the interpreter and once using the JITed code

A wrapper program (in the \texttt{ocaml-jit-tools} crate) runs a specified program with
tracing enabled twice simultaneously - one run uses the JIT and the other the
existing interpreter.  Then for every trace entry received it compares the two lines. If there is a
difference between the lines it shows a diff and then exits, as shown in Figure
\ref{fig:trace-comparison}.

As many of the values are pointers there is a risk of non-determinism making this comparison fail.
I used a small open source wrapper program called \texttt{no-aslr} to disable ASLR\footnote{GPLv2
      licensed, \url{https://github.com/kccqzy/no-aslr}}. In order to ensure
that the Rust code doesn't cause them to become unaligned I ran the compiler regardless of whether
JITed code was enabled when tracing was enabled. These two things together worked well enough that
all of the memory addresses were aligned and deterministic. This is unlikely to be true in general
for all OS kernels and malloc implementations but worked on Linux with glibc.

The only expected difference comes from the use of the machine PC rather than the bytecode PC --
instruction pointers, like return addresses on the stack, could differ. This required a special
case during the check.

I added a script to 11 test programs that together mostly covered the entire instruction
set. Running this frequently allowed me to test for regressions when making changes.

\begin{figure}[h]
      \includegraphics[width=\textwidth]{trace-comparison}
      \caption{Output on trace comparison failure}
      \label{fig:trace-comparison}
\end{figure}

\subsubsection{Towards full correctness}

Once I was happy that I had implemented every instruction, I started using
the OCaml compiler's internal test suite. I discovered some subtle bugs and used it to add new test
programs and fix them by trace comparison. One test heavily used callbacks from C to OCaml and I
discovered my initial implementation was too slow.

I eventually managed to get nearly all tests in the test suite working - the only failures were
testing the backtrace support and the debugger.

After this I successfully managed to bootstrap the compiler using the JIT which gave me a high
level
of confidence in the accuracy of the JIT-compiled code.

\subsection{Omitted details}

Although the basic structure as described holds, some details are omitted but appear in
\texttt{emit\_code.rs}.

\begin{itemize}
      \item There is extensive optional support for tracing instructions and events)
      \item Callbacks from C to OCaml code require special handling
      \item The compiler returns a pointer to the first instruction to support OCaml's
            metaprogramming \texttt{ocaml\_reify\_bytecode} program used in the implementation of
            things like the
            toplevel \texttt{ocaml} program.
      \item Function application is a fairly involved process and there are checks to resize the
            stack and check for signals as well as the fundamental complexity of the push-enter
            model
      \item Registers need to be saved and restored at safepoints to allow the garbage collector to
            use them
      \item Certain operations (like \texttt{ClosureRec(...)}) are involved enough that instead of
            inlining the definition in
            hand-written assembly, I push the registers to the C stack and call a C primitive to
            implement the
            operation (taking the registers as a struct)
      \item The compiler stores a persistent data structure of the sections to allow mapping
            bytecode addresses to machine-code addresses and allow for clean-up after a section is
            freed
\end{itemize}

\section{Dynamic recompilation} \label{dyn-recomp}

Although the initial compiler there is nothing that could not inherently be done ahead-of-time with
some more work on linking. The main benefit to operating as a JIT is it allows for OCaml toplevel
sessions to be JIT-compiled. Adding dynamic recompilation allows for the system to discover hot
paths in the code and optimise them more which is something that requires information about runtime
behaviour. This is a popular technique for many existing optimising JITs like V8.

The structure of the bytecode means it is natural to make the optimised compiler operate at the
granularity of a whole function, rather than a basic block or execution trace. Counting calls and
only branching to the optimising compiler above a certain threshold is a natural method for
determining hot functions and the method I decided to use.

However, allowing this required some changes to the initial compiler. In order to explain these
changes I first give an overview of how the OCaml interpreter deals with efficient
implementation of multiple arguments in the context of a functional language with partial
application.

\subsection{Existing OCaml calls} \label{exist-ocaml}

All functions in OCaml are implemented as closures - a heap allocated tuple of a code pointer and a
list of data. Function calls are implemented as:

\begin{minted}{c}
check_stacks_and_signals(regs);
struct closure *c = (struct closure *) regs->env;
call_function(regs, f->code_ptr);
\end{minted}

Note the actual call logic is performed in hand-written assembly and these structs do not exist
anywhere
- this and subsequent examples are just pseudo-code. However, it is easier to understand the logic
at a higher level. \texttt{regs} is a pointer to a struct containing the OCaml registers - in the
actual assembly these are machine registers.

\subsubsection{Partial applications}

OCaml uses curried arguments allowing for the use of partial application:

\begin{minted}{ocaml}
# let add a b = a + b;;
val add : int -> int -> int = <fun>
# add 2 3;;
- : int = 5
# let add3 = add 3;;
val add3: int -> int = <fun>
# add3 5;;
- : int = 8
\end{minted}

In order to avoid making too many intermediate closures, OCaml has special support for this
extensively used pattern. All arguments are passed in order on the stack and the
\texttt{extra\_args}
register is used to mark how many arguments were used.

The number of arguments passed minus 1 is stored in the \texttt{extra\_args} register. The compiler
will insert a \texttt{GRAB(req\_extra\_args)} instruction at the start of any functions that take
more than one argument.  This instruction will cause an early return from the function if
\texttt{extra\_args} <
\texttt{passed\_extra\_args}. The return value will be a closure itself - this represents partial
application of curried functions. The data of this closure will be the arguments that were passed.
The code pointer of this returned closure points to a \texttt{RESTART} opcode immediately
preceding the grab, which moves the passed arguments off  the closure's data section onto the
stack, sliding them on top of any additional arguments passed.	Otherwise, it will subtract
\texttt{required\_extra\_args} from the \texttt{extra\_args} register
and continue executing.

This model is categorised as a push-enter model because it is the responsibility of the callee to
deal with arity mismatches.

\subsection{Function table}

In order to support dynamic recompilation it is necessary to store the call count somewhere. A
naive method would be to store this in the closure as an extra data item, but this cannot support
optimising cases where the same code is used with different closure environments.

Instead, I modified the code pointer to point to a structure in memory. This structure is shared
between all closures using the same code. For now imagine it contains only one field: the code
pointer. Calls become:

\begin{minted}{c}
check_stacks_and_signals(regs);
struct closure *c = (struct closure *) regs->env;
struct function_metadata *f = c->code_ptr;
call_function(regs, f->code);
\end{minted}

\subsection{Moving the responsibility to the caller - eval/apply model}

I do not wish for optimised functions to do any of the work described in section \ref{exist-ocaml}.
As is described later, most of the speed increases of the optimised compiler comes from completely
eliminating use of the OCaml stack and the call method above makes heavy use of it.

Instead I shift the work to logically be performed by the caller. The way this is achieved is by
adding a
field storing \texttt{required\_extra\_args} to the function table and transforming the function
call to look something like this:

\begin{minted}{c}
check_stacks_and_signals(regs);
struct closure *c = (struct closure *) regs->env;
struct function_metadata *f = c->code_ptr;
if (regs->extra_args < f->required_extra_args) {
      // return partial application doing work of GRAB
} else {
      regs->extra_args -= f->required_extra_args;
      call_function(regs, f->code);
}
\end{minted}

I then modified the assembly translation of \texttt{GRAB} to be a no-op. Identical work is
performed but now it is the responsibility of the caller.

\subsection{Call counts and dynamic recompilation} \label{final-call-logic}

I added two new fields to the function metadata struct: a reference to the original bytecode
location
of the function and a count/status variable.  The count/status is initialised to 0. Calls become:

\begin{minted}{c}
check_stacks_and_signals(regs);
struct closure *c = (struct closure *) regs->env;
struct function_metadata *f = c->code_ptr;
if (regs->extra_args < f->required_extra_args) {
      // return partial application doing work of GRAB
} else { 
      regs->extra_args -= f->required_extra_args;
      if (f->count_status == -1) {
            // Call the optimised function
            call_opt(regs, f->code);
      } else if (f->count_status == THRESHOLD_CALLS) {
            // Run the optimising compiler then call the optimised function
            f->code = optimising_compiler(f->original_bytecode_location);
            f->count_status = -1;
            call_opt(regs, f->code);
      } else {
            // Increment count and call the unoptimised function 
            f->count_status++;
            call_function(regs, f->code);
      }
}
\end{minted}

\subsection{Actual implementation}

Even though most of the above snippets are pseudo-code, the function metadata struct as listed with
its four fields (code pointer, original bytecode location, required extra args, count/status)
exists in the compilation. Before implementing the optmising compiler, the initial compiler was
modified to generate, store and use it.

In order to generate this table, I added another pass to the initial compiler that discovers the
bytecode locations of all closures. This is performed by iterating through all functions. When a
\texttt{Closure(bytecode\_location, num\_vars)} bytecode operation is found (which creates a
closure) the code pointer is inspected. By inspecting the first instruction found at the bytecode
location, it is possible to find the value of the \texttt{required\_extra\_args} field.

After all the code is emitted, the function table is emitted. \texttt{dynasm-rs}'s relocations are
used to load the initial code pointers into the function table and link to the struct in the
closure table when compiling \texttt{Closure(...)} operations.

The net result of these changes is an extra pointer lookup being done every time a function is
called. However this is necessary to allow dynamic replacement of functions with optimised
versions.

\section{Optimising compiler} \label{opt-comp}

The previous section a way to determine hot closures and dynamically call
into an optimising compiler. This section describes the implementation of this optimising compiler
and how calls are done.

The time left for the prjoect meant building a complete x86\_64 compiler backend (with register
allocation,
instruction scheduling, etc.) from scratch was infeasible. The usual toolkit used to avoid
replicating this work is LLVM \cite{llvm}. Although is more typically known for its use in
ahead-of-time (AOT) compilation, there is some support for use in JITs. However, its large size and
complexity and AOT focus made it difficult to integrate.

\subsubsection{Cranelift}

Instead I used the \texttt{cranelift} project. Like LLVM it is a retargetable code generator with
an IR. However, it has a number of different design decisions which made it much better suited for
my planned uses:

\begin{itemize}
      \item It is written in Rust which means the API is fairly idiomatic when using it in Rust
      \item It is designed primarily for JITs and focuses heavily on compilation performance
      \item The project is actively developed and I could communicate with the developers online
      \item Although the support for garbage collection is not as extensible, with the help of the
            developers I was able to come up with a model that is a good fit for the OCaml garbage
            collector's
            requirements.
      \item When I encountered any bugs or missing features I could easily submit and get merged
            patches to the project which uses a familiar Github workflow rather than the LLVM
            project's legacy
            systems.
\end{itemize}

There were still some missing features in Cranelift which are described later but for the most
part it was a good fit for my needs and was in a large part responsible for my being able to
complete this ambitious extension.

\subsection{Overview}

At a high level the optimised compiler is a function taking two inputs and returning two outputs.
The inputs are a slice (Rust $(\text{pointer}, \text{length})$ tuple representing a reference to an
array) to a bytecode, and the offset within that slice containing the first instruction in the
function to be optimised.

The first output is a pointer to a compiled function that can be called to execute the code
represented by that function. The second output is a vector of stack map entries which are used
to support finding GC roots (explained in \ref{optgc}). The first output becomes the new code
pointer in the function metadata table.

The compiler consists of three major components in a pipeline.

\begin{enumerate}
      \item Basic block conversion and stack-start analysis (section \ref{opt-bb})
      \item IR generation from the basic blocks using the analysis (section \ref{opt-irgen})
      \item Compilation of IR to x86\_64 assembly (performed by Cranelift)
\end{enumerate}

As described later in section \ref{dyn-recomp}, this compiler is triggered once a function is
called enough times to become `hot'. After the function has been compiled the optimised
implementation
will always be used.

\subsection{Optimisations done}

The largest change from the initial compiler to the optimised compiler is that it no longer
generates
code that uses the OCaml stack and accumulator registers. This is achieved by replacing these uses
of the OCaml stack with machine registers which spill to the C stack when needed.

Arguments are passed to a function according to the System V C calling convention - the first
argument is always the closure environment, \texttt{env}, and the remaining arguments are passed
in registers. Direct calls from optimised functions to optimised functions can avoid the use of
the OCaml stack entirely. Other cases need more work (described in section \ref{dyn-recomp}).

The method used to remove the use of the stack is to convert the OCaml bytecode into an SSA form
by keeping track of the contents of a virtual stack and accumulator as the closure is translated.
As an example consider the simple OCaml function:

\mint{ocaml}|let add a b = a + b|

This compiles to the sequence of bytecode instructions:

%TC:ignore
\begin{verbatim}
Grab(1)          # Ensure 2 arguments passed
Acc(1)           # Load 2nd element from top of stack to acc
Push             # Push acc onto top of stack
Acc(1)           # Load new 2nd element from top of stack to acc
ArithInt(Add)    # Add the top of stack to the acc and pop it
Return(2)        # Pop 2 items, then return acc
\end{verbatim}
%TC:endignore

Instead of translating all of these inefficient stack manipulations as the initial compiler does,
the optimised compiler keeps track of the stack as it compiles. For an example of this, see Table
\ref{table:stacktrans}. Ignore the \texttt{Grab(1)} for
now as it is handled in a different way.

\begin{table}[h]
      \centering
      \begin{tabular}{ccc}\toprule
            Initial state                   & Operation              & Final state
            \\
            \midrule
            \\
            \texttt{acc=?, stack=[b, a]}    & \texttt{Acc(1)}        & \texttt{acc=a, stack=[b, a]}
            \\
            \texttt{acc=a, stack=[b, a]}    & \texttt{Push}          & \texttt{acc=a, stack=[a, b,
                              a]}
            \\
            \texttt{acc=a, stack=[a, b, a]} & \texttt{Acc(1)}        & \texttt{acc=b, stack=[a, b,
                              a]}
            \\
            \texttt{acc=b, stack=[a, b, a]} & \texttt{ArithInt(Add)} & \texttt{acc=a+b, stack=[b,
                              a]}
            \\
            \texttt{acc=a+b, stack=[b, a]}  & \texttt{Return(2)}     & \texttt{acc=a+b, stack=[]}
            \\
            \bottomrule
      \end{tabular}

      \caption{Translation of add}
      \label{table:stacktrans}
\end{table}

Rather than store expressions in the virtualised state, all values use cranelift's concept of an
SSA value.

This method is effective when considering a single basic block at a time. However, in the presence
of branching, joining and looping basic blocks more work is needed. Luckily the output of the OCaml
compiler has the invariant that the stack size relative to the function's stack frame is constant
at the start of each basic block, \emph{no matter the path taken to reach it}. This happens to be
the key to allow efficient SSA conversion, completely removing any explicit notion of a stack and
accumulator from the generated Cranelift IR.

\codesubsection{Conversion to basic blocks}{src/rust/ocaml-jit-shared/src/basic\_blocks/}
\label{opt-bb}

The typical data structure used in optimising compilers is the basic block. Cranelift is no
exception and the IR for a function consists of basic blocks containing instructions.

However, the only information we have from the bytecode is a sequence of instructions with jumps.
In order to move away from a heavy connection with the instructions the first step is to group them
into
basic blocks.

\codesubsubsection{Types}{basic\_blocks/types.rs}

A \textbf{\texttt{BasicClosure}} consists of a vector of \textbf{\texttt{BasicBlock}} along with
some other data. A basic block has a vector of \textbf{\texttt{BasicBlockInstruction}} and a single
\textbf{\texttt{BasicBlockExit}}.

Some \texttt{Instruction}s map to a \texttt{BasicBlockInstruction} and others map to a
\texttt{BasicBlockExit}. For example, \texttt{Instruction::Push} becomes
\texttt{BasicBlockInstruction::Push} but \texttt{Instruction::BranchIf(label)} becomes
\texttt{BasicBlockExit::BranchIf { then\_block, else\_block }}.

This enforces at the type level the basic invariant of basic blocks - one entry, one exit.

\subsubsection{Stack starts}

In addition to performing the conversion to basic blocks, the algorithm also keeps track of a
concept I call the \textbf{stack start} of a basic block. Although, like all other aspects of the
OCaml interpreter, it is not documented anywhere, inspecting the bytecode compiler's source and
testing against all real programs finds an important invariant:

\begin{framed}
      \noindent
      For all paths leading from the entry block of a function to an instruction, the absolute
      stack size
      relative to the start of the function's stack frame is the same.
\end{framed}

During the conversion to basic blocks this invariant is validated and the size of the stack before
the first instruction of a block is stored. This is the \textbf{stack start} of the block.

\codesubsubsection{Algorithm}{basic\_blocks/conversion.rs}

The algorithm consists of two iterations of depth first search (DFS). The first finds all bytecode
offsets that form the start of blocks. The second visits all these blocks working out stack sizes.

In order to describe this algorithm without too many OCaml-specific confusing details I will
demonstrate it with
a simplified model:

The first DFS pass is only necessary to deal with loop back edges. For simplicity let's assume this
doesn't happen.

Let \(I\) be the set of all instructions. The program \(P\) is a list of instructions and can be
indexed. \(P\) can be thought of as a function \(\mathbb{N} \to I\).
For simplicity, assume indices are consecutive\footnote{They're not, but it's fairly simple to
      handle}.

Each instruction has four properties. Let $\mathbb{B} = \{ \textbf{true}, \textbf{false} \}$:

\begin{itemize}
      \item \(\delta(i) : I \to \mathbb{N} \) is the change to the stack pointer after executing
            the
            instruction. For
            example, \texttt{Push}, which pushes the acc to the stack has, \(\delta(\texttt{Push})
            =
            +1\)
      \item \(\dfsexit(i) : I \to \mathbb{B} \) is \textbf{true} iff the instruction causes
            the block to end
            (jumps, returns)
      \item \(\dfsfallthrough(i) : I \to \mathbb{B} \) is \textbf{true} iff \(\dfsexit(i)\)
            and it can end up
            jumping to the following instruction. \texttt{BranchIf(branch\_loc)} is fallthrough but
            \texttt{Branch(branch\_loc)} is not.
      \item \(\dfslabels(i) : I \to \mathcal{P}(\mathbb{N}) \) is the set of all labels the
            instruction could end up jumping to
            (not
            including falling through). For example, \(\dfslabels(\texttt{BranchIf(branch\_loc)}) =
            \{\text{branch\_loc}\}\). It is only relevant if \(\dfsexit(i)\)
\end{itemize}

Under this model, the algorithm to find all the blocks and their starts is shown in algorithm
\ref{alg-dfs}.

\begin{algorithm}
      \caption{DFS to find basic blocks and their starts}\label{alg-dfs}
      \begin{algorithmic}[1]
            \Function{FindBlocks}{$P, arity$}
            \State $seen \gets \emptyset$
            \State $starts \gets \{\}$
            \Function{VisitBlock}{$initial\_index, initial\_stack\_size$}
            \If{$initial\_index \in seen$}
            \Assert{$starts[initial\_index] = initial\_stack\_size$}
            \State \textbf{return}
            \EndIf
            \State $starts[initial\_index] \gets initial\_stack\_size$
            \State $i \gets initial\_index$
            \State $s \gets initial\_stack\_size$
            \Repeat
            \State $x \gets P[i]$
            \State $s \gets s + \delta(x)$
            \If{$\dfsexit(x)$}
            \For{$j \in \dfslabels(i)$}
            \State \Call{VisitBlock}{j, s}
            \EndFor
            \If{$\dfsfallthrough(x)$}
            \State \Call{VisitBlock}{i + 1, s}
            \EndIf
            \State \textbf{return}
            \EndIf
            \State $i \gets i + 1$
            \Until{forever}
            \EndFunction
            \State \Call{VisitBlock}{0, arity}
            \State \textbf{return} $starts$
            \EndFunction
      \end{algorithmic}
\end{algorithm}

\subsubsection{Additional operations performed during the search} \label{addop}

The algorithm, as presented in Algorithm \ref{alg-dfs}, shows only the basic logic of the search
and
storing of the stack starts for each block. The actual search also performs more work:

\begin{itemize}
      \item When a block is visited, a vector is created to hold the parsed instructions
      \item Blocks are numbered in reverse post-order (the natural order for basic blocks where,
            with the exception of loop back edges, blocks are visited in a way consistent with
            execution order).
      \item Every time an instruction is visited any labels referencing other blocks are replaced
            with the block ids and a copy is added to the vector.
      \item At exit, all of the block metadata is stored in a \texttt{BasicBlock} struct and placed
            in a table of blocks referenced by the block number.
      \item The maximum stack size at any point in the function is computed. It is used later in
            the IR generation phase.
\end{itemize}

\subsubsection{Example}

There isn't space to fit it in the main body of the text but Appendix \ref{appendix-example} shows
an example of the output of the basic block conversion for a larger function with if statements.

\subsection{Calling conventions}

In the initial interpreter, all calling is performed using the OCaml interpreter's existing model.
This
passes all arguments on the OCaml stack and has the caller deal with partial application and tail
calls.

We would like to avoid the stack in hot paths, so instead use a model where arguments are passed
according to the
System V calling convention. This is enabled by the changes made in Section \ref{final-call-logic}.

Values stored in the closure's environment need to be accessible to the function, so the first
argument is always a reference to the closure's environment. The remaining arguments correspond to
the function's arguments when fully applied with the arguments it expects.

The function returns two values: the return value of the function and another value which
corresponds to an \texttt{extra\_args} offset used for doing tail calls.

For example,

\mint{ocaml}|let add a b = a + b|

\noindent
is translated to a function with signature

\[(\text{env: i64}, \text{a : i64}, \text{b : i64}) -> (\text{i64}, \text{i64})\]

\noindent
I limit the compiler to only work with functions of up to 5 arguments, which corresponds to the
maximum number of registers that can be used for arguments before needing to use the stack in the
System V calling convention. This simplified the interface code and in practice nearly every
function in the standard library can work in this way.

Calls from from optimised code to optimised code can be implemented with simple C function calls.
Calls from the stack-using output of the initial compiler require the caller to read the arguments
from the stack and call the function with this convention. Calls from optimised code to unoptimised
code require writing the arguments back to the OCaml stack. All of these cases are implemented in
hand-written assembly which checks first for the optimised->optimised case before falling back on
slower
stack-using operations. This allows for the hot path sequence of calls to be optimised.

\subsection{IR generation} \label{opt-irgen}

Cranelift, much like LLVM, uses an IR with a binary, in-memory and textual representation. The
basic concept is that of typed values where the types are things like \texttt{I64} and
\texttt{F32}.

IR generation proceeds by initialising the function header and iterating over every basic block
emitting cranelift IR.

When constructing IR as part of the frontend, cranelift provides some assistance with
translating mutable variables to SSA values as the IR is being emitted block parameters.
This is similar to LLVM's \texttt{mem2reg} pass but more direct and runs online with the
construction
of the SSA.

This functionality is the key method for removing use of the OCaml stack and accumulator. Cranelift
\texttt{Variable}s are defined to correspond to each stack location used in the function and the
accumulator.
Each variable needs a unique integer id allowing Cranelift's instruction builder will then track
the
flow of values as the IR is being emitted.

\begin{minted}{rust}
let acc_var = Variable::new(0);
builder.declare_var(acc_var, types::R64);
let stack_vars: Vec<_> = (0..max_stack_size).map(|i| {
    let v = Variable::new(i + 1));
    builder.declare_var(acc, types::R64);
    v
}).collect();
\end{minted}

As the function is compiled we keep track of the current stack size: the location of the stack
pointer relative to the start of the frame. This is the purpose of computing the stack starts
earlier -- whenever we start emitting code for a basic block we can set the current stack size to
what it would be on block entry.

As we generate code, we can get a reference to the current value of a variable or set a new value.
The IR builder will take care of tracking this through the control flow we define automatically
inserting the minimum number of block parameters needed to deal with different values being
assigned
to a variable in multiple different entry blocks.

For example, \texttt{PUSH} (push current accumulator to stack) is handled in this way:

\begin{minted}{rust}
let current_acc = self.builder.use_var(self.acc_var);
self.builder.def_var(self.stack_vars[self.current_stack_size], current_acc);
self.current_stack_size += 1;
\end{minted}

In order to simplify this, I have defined utility methods for operations like pushing to and
popping from the stack and modifying the accumulator.

\subsubsection{Example: add}

Let's consider the \texttt{ArithOp(Add)} example again. It adds the top of the stack to the current
accumulator value (where adding integers in OCaml requires a decrement).

\begin{minted}{rust}
let a = self.get_acc_int();  // my utilities wrapping def_var and use_var
let b = self.pick_int(0)?;
self.pop(1)?;

// a is OCaml rep of x, b is OCaml rep of y
// a + b = (x * 2 + 1) + (y * 2 + 1) = (x + y) * 2 + 2
// result = a + b - 1 = (x + y) * 2 + 1
let added = self.builder.ins().iadd(a, b);
let result = self.builder.ins().iadd_imm(added, -1);
self.set_acc_int(result);
\end{minted}

The code implementing this case is remarkably similar to the original interpreter - however rather
than directly performing the operation it is instead making calls that trigger the emission of IR
with the same semantics.

This is true also of the implementation of the other operations. Each instruction can be written in
isolation using the concept of the stack pointer and accumulator, but rather than modify these at
runtime we keep track of the state of these registers as we compile.

\subsubsection{Complete example}

To show this example in complete context consider the add function:

%TC:ignore
\mint{ocaml}|let add a b = a + b|

\begin{verbatim}
Arity: 2
Max stack size: 3

# Block 0 (stack_start = 2)
Acc(1)
Push
Acc(1)
ArithInt(Add)
Exit: Return(2)
\end{verbatim}
%TC:endignore

As it is an OCaml function taking two arguments, it is compiled to a machine function taking 3
arguments - the closure environment (unused for this function) and the two original arguments,
\texttt{a} and \texttt{b}.

It compiles to something like this\footnote{the actual IR is larger as it uses \texttt{R64} types
      and bitcasts for GC as
      described in section \ref{gc-ir} but the principle is the same}:

%TC:ignore
\begin{verbatim}
function u0:0(i64, i64, i64) -> i64, i64 system_v {

block0(v0: i64, v1: i64, v2: i64):
    v3 = iconst.i64 0
    v4 = iadd v1, v2
    v5 = iconst.i64 -1
    v6 = iadd v4, v5
    jump block1

block1:
    return v6, v3
}
\end{verbatim}
%TC:endignore

Despite the original function and the translation code making use of the stack and accumulator,
there is no site of this in the emitted IR. Values pass directly from the function parameters to
nodes representing the arithmetic operations and in to the return values.

\subsubsection{Larger example}

Appendix \ref{appendix-example} shows the translation for a function with multiple blocks where
variables could have different values due to the branching and merging of control flow due to if
statements. Note that at block 5 has a block parameter defined for the return value and jumps to it
pass the specific values it could take. Inserting this block parameter was done automatically once
cranelift determined it was needed.

\subsection{Machine code translation}

Cranelift compiles the IR to this machine code. Note: in the System V calling convention, the first
three arguments are passed in \texttt{rdi} and \texttt{rsi}, \texttt{rdx} respectively and return
values (up to 2) are in \texttt{rax} and \texttt{rdx} respectively:

%TC:ignore
\begin{verbatim}
0000000000000000 <arith_add>:
   0:	55           push   rbp                    ; set up rbp chain
   1:	48 89 e5     mov    rbp,rsp                ;  
   4:	48 01 d6     add    rsi,rdx                ; v4 = v1 + v2
   7:	48 83 c6 ff  add    rsi,0xffffffffffffffff ; v6 = v4 + (v5 = -1)
   b:	48 89 f0     mov    rax,rsi                ; 1st retval = v6
   e:	48 31 d2     xor    rdx,rdx                ; 2nd retval = v3 = 0
  11:	48 89 ec     mov    rsp,rbp                ; restore rbp
  14:	5d           pop    rbp                    ;
  15:	c3           ret    
\end{verbatim}
%TC:endignore

This assembly is high quality and uses significantly less instructions than the output of the
compiler would need. No memory accesses are done at all outside of the entry and exits, compared to
the multiple reads and writes to the OCaml stack in the original instruction.

\subsubsection{Summary}

The combination of Cranelift and my virtual stack allows me to write code which is in mostly 1-1
correspondence with the interpreter source while compiling to register-using optimised machine
code. Cranelift's optimisation for JIT use cases means the stages of the pipeline are fast - the
register allocator uses linear scan and only simple and quick optimisation passes are done on the
IR.

\subsection{GC support} \label{gc-support}

A key aspect of the OCaml runtime is its garbage collection support. In the bytecode interpreter no
particular effort is needed for this as every value is stored on the OCaml stack. However, for my
compiled code, which does not use the OCaml stack, it is necessary to build something more
involved:

\begin{enumerate}
      \item During cranelift IR generation, I store everything that could contain a pointer to a
            GC-managed value (a \textbf{local root}) in a cranelift \textbf{reference type}
            (\texttt{R64}).
      \item Cranelift performs live variable analysis, spilling and restoring of registers to the C
            stack and emission of \textbf{stack maps} at every GC \textbf{safepoint}.
      \item My runtime stores the stack maps in a hash map keyed by the return address at time of
            the safepoint.
      \item During GC, my runtime walks the entire frame pointer (\texttt{rbp}) chain. By looking
            at the return address and using the hash map to discover any stack maps, my runtime
            tells the OCaml GC about the location and value of any local roots that could contain
            pointers.
\end{enumerate}

\subsubsection{IR generation} \label{gc-ir}

Cranelift has support for precise garbage collection. It was originally created for webassembly
reference types but luckily mapped well to the needs of OCaml. Precise (sometimes called
\emph{exact} or \emph{accurate}) means that the GC is able to determine all roots and only the
roots
when tracing. The opposite is a \emph{conservative} collector, which allows for an over-estimate.

In order to use it my IR generation has to mark roots by storing them in values that have the
\texttt{R64} type. The \texttt{R64} type means a 64-bit reference and is compiled identically to
\texttt{I64}. However, cranelift support will use this type to differentiate GC-managed pointers
from other values.

For this reason, anything that is an OCaml value and could be a pointer to the heap \textbf{must}
be stored in a \texttt{R64} type at every point the GC could trigger (a \emph{safepoint}).

However, despite being almost equivalent in implementation to \texttt{I64}s not all operations that
can be done using \texttt{I64} values can be performed using \texttt{R64} values. This is only
really a
problem because of OCaml's mixed integer/pointer data representation. The integer add operation is
not
implemented for \texttt{R64}s but is needed to add two integers stored in OCaml's uniform
representation.

I used cranelift's \texttt{raw\_bitcast} (which does C-style type casting at the IR level) to
temporarily convert \texttt{R64} values to \texttt{I64}, perform any arithmetic, and then convert
back.

The key invariant I had to hold to make this work was that these typecast \texttt{R64}-derived
\texttt{I64} were only alive for ranges shorter than the range between functions which could
trigger
garbage collection.

\subsubsection{What cranelift does}

Cranelift's GC support works using the concept of safepoints. A safepoint is a point in the program
where the garbage collector could run. Cranelift treats every function call as a
safepoint\footnote{I planned a way to make this configurable per-function with the cranelift
      developers but decided it would take too long at the late stage of the project}.

At every safepoint (here function call), cranelift will ensure:

\begin{enumerate}
      \item All live \texttt{R64} values have a copy on the stack so the GC could find them
      \item No code or optimisation assumes that the old value of a \texttt{R64} is still valid
            after the safepoint
\end{enumerate}

In order to determine the live reference types at every safepoint, cranelift performs a live
variable analysis (LVA) pass. It then will spill and restore (push and pop) any reference
type-containing machine registers to the C stack (other values will be on the stack already).

This spilling takes place as part of register allocation. As is typical for optimised solvers of
NP-complete problems, cranelift's register allocator is thousands of lines of incredibly dense
algorithmic code that I do not fully understand.

\subsubsection{Stack maps}

The output of this step is calls to a callback handler I provide with \((\text{native code offset},
\text{stackmap})\)
tuples.

The stack map is a set of offsets relative to the stack pointer at time of call where reference
typed values are located. To store these entries efficiently cranelift uses a bitset data
structure. The interpretation of these offsets is shown in the following diagram (where \texttt{x}
is each member of the stack map's set):

%TC:ignore
\begin{verbatim}
          Stack
        +-------------------+
        | Frame 0           |
        |                   |
   |    |                   |
   |    +-------------------+ <--- Frame 0's SP
   |    | Frame 1           |
 Grows  |                   |
 down   |                   |
   |    | Live GC reference | --+--
   |    |                   |   |
   |    |                   |   |
   V    |                   |   x = offset of live GC ref
        |                   |   |
        |                   |   |
        +-------------------+ --+--  <--- Frame 1's SP
        | Frame 2           |
        | ...               |
\end{verbatim}

\noindent\emph{
      This diagram is directly copied from Nick Fitzgerald's blog post \cite{refblog} describing
      cranelift's GC. License: CC-BY-SA.
}
%TC:endignore

\subsubsection{Integration with OCaml}

After compilation is finished, I translate the nativeicode offsets to an absolute address by adding
the
pointer to the first instruction in the compiled function. I store all stackmaps in a hashmap
indexed by the native code offset.

During garbage collection, the garbage collector has points where it scans all of the "local
roots". It passes a callback function that takes two arguments: the address of the root in
memory
and the value at that address.

In order to use my return address map, it is necessary to walk up the chain of stack frames in the
same way as a debugger does.  One way I initially tried was to use Apple's \texttt{libunwind}
library which is capable of doing this and used for things like C++ exception handling or Rust
backtraces on panic. However, integrating this with a JIT is involves emitting DWARF debug
information and registering them against the runtime. There is no consolidated documentation for
any
of this. Wasmtime/cranelift have actually done this work for their own use of the GC. However, I
would have to manually do it for every function in the initial compiler which is not really
work that can be done by a human.

I instead settled on a less complicated solution: use frame pointers. Frame pointer chains are`
optional in \texttt{x86\_64} calling conventions and usually omitted by optimising compilers.
However, by telling the Rust compilers and GCC to not emit frame pointers, and manually adding the
\texttt{rbp} chain assembly to my initial compiler's output I could ensure that a linked list
repeatedly dereferencing the initial \texttt{rbp} value would walk the entire stack. The code to do
this is as follows:

\begin{minted}{c}
typedef void (*scanning_action) (value, value *);

CAMLexport void jit_support_scan_bp(scanning_action f) {
  uint64_t **bp;
  asm ("mov %%rbp, %0;" : "=r" (bp));

  while(bp != 0) {
    rust_jit_lookup_stack_maps(bp + 1, f);
    bp = (uint64_t **) *bp;
  }
}
\end{minted}

The \texttt{rust\_jit\_lookup\_stack\_maps} function performs a search in the hashmap on the return
address
($\texttt{bp} + 1$). If it finds a stack map, it will use the information to call \texttt{f} on
each root found.

\subsubsection{Summary}

Garbage collection is incredibly difficult to get right. Luckily, cranelift had some support for it
I could reuse. My use of it was non-standard but ultimately successful. Once I had this I needed a
way to walk through the C stack. Initial approaches were unsuccessful but ultimately falling back
to use of the \texttt{-fno-omit-frame-pointer} compile flag worked well.

I paid some performance penalty by using frame pointers and my hash map for looking up stack frames
was unoptimised. However, once all components were in place the system worked remarkably well.

\subsection{Exception handling}

Exception handling was a particularly intricate detail to the project and explaining all the
intricacies of how OCaml deals with exceptions would take many pages. In fact, the only bytecode
instruction unsupported by the optimised compiler is the \texttt{PushTrap} operation, which is used
in implementing \texttt{try-catch} operations.\footnote{One particularly hard to debug problem led
      to a 2003 bug
      report in French where Xavier Leroy thanked the reporter for a particularly `interesting'
      problem
      to fix. I had unknowingly re-introduced the conditions that led to that bug and unfortunately
      did not find it as interesting to fix.}

I will cover one

At a very high level, exception support in the existing interpreter is complicated by the fact the
interpreter could call a C primitive that itself calls back into the interpreter in a different C
frame. To support this, OCaml uses C \texttt{sigsetjmp} and \texttt{siglongjmp} functions. These
functions work by saving and restoring the values of all machine registers and POSIX signal masks
to a buffer, allowing for `long' jumps up the stack.

Most OCaml exception raises do not require a longjmp as they do not need to pass through
the callback stack. However, my optimised compiler emits most functions as C functions and in
general exception raises do require a longjmp.

The buffer used to store the state to restore is rather large and in the naive implementation would
need to be allocated by \emph{every} function on the stack. This was a clear issue needing solving
if there was any hope of the optimised compiler actually being faster.

My solution was to note that I only needed two registers to completely restore my interpreter's
state - \texttt{rbp} and the x86\_64 instruction pointer. This is because the only places the
longjmp could point to are at specific locations where all other registers can be seeded by other
means. I ended up writing my own jump code to do this:\footnote{Note inline gcc assembly is AT\&T
      not Intel
      syntax as I have used for the rest of the project}

\begin{minted}{c}
asm (
      "movq %0, %%rbp\n"
      "jmp *%1"
      :
      : "r" (buf->data.asm_saved.bp)
      , "r" (buf->data.asm_saved.pc)
);
\end{minted}

\subsubsection{Implementation and testing}

I used a similar method to that described in Section \ref{trace-comparison} with the same test
files
when implementing and debugging the compiler. Instead of tracing on every instruction, it only
tested
passed arguments on entry to functions and return values from functions as well as arguments and
results of
primitive C functions called \texttt{CCall(...)}.

After tests were working the OCaml test suite was used for verification. The results are described
in Section
\ref{eval-opt-comp-qual}.

\subsection{Limitations of Cranelift}

Despite allowing me to achieve this incredibly complicated system in a relatively short amount of
time, Cranelift is a young project which is not without its flaws.

\begin{enumerate}
      \item Cranelift has no support for tail calls yet. I had to instead perform tail calls in a
            hand-written wrapper function.
      \item There is no way to mark a primitive (such as a call to OCaml's GC write barrier,
            \texttt{caml\_modify}) as not requiring a safepoint which means the emitted assembly
            tends to have
            too many spills
      \item Despite my \texttt{R64} $\leftrightarrow$ \texttt{I64} workaround mostly working, I
            managed to trigger some obscure bugs in the register allocator by the non-standard use
            of reference
            types.
      \item Cranelift has limited support for other calling conventions, or operating outside of
            the context of a C function.
      \item Exception/trap handling is difficult in cranelift and essentially requires the use of
            libunwind. As such I do not support catching exceptions in optimised code
\end{enumerate}

However, despite this, I had a much easier time integrating cranelift than LLVM. I think its core
design decisions make it fill a unique niche where LLVM cannot. I have no hesitations in
recommending it for other Rust JIT projects.