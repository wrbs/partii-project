% !TeX root = ../dissertation.tex
\chapter{Implementation}

\section{Architectual Overview}

\input{graphs/arch.tex}

I have implemented a Rust static library which is linked in to the OCaml runtime library and
\texttt{ocamlrun} (the interpreter).
\texttt{ocamlrun} hooks into this library when it loads bytecode and when it starts
interpreting it.
If the JIT is enabled (either by setting an environment variable or enabling it by default at
compile time),
when the bytecode load hook is run the \textbf{initial compiler} will execute.

The initial compiler parses the bytecode into a stream of instructions and for each bytecode
instruction
it emits assembly with the same semantics to a buffer. After all code has been emitted (including
headers
and footers with shared routines) relocations are performed and the buffer is marked as executable.

When \texttt{ocamlrun} then calls the hook to start interpretation, the library instead jumps to
this assembly code which then performs the same operations as the interpreter - except that every
instruction has effectively been inlined.

When an OCaml closure is applied\footnote{all non-primitive function calls are translated to
    closure application}
a closure execution count is incremented. Once this count passes a configurable threshold the code
instead
branches in to the \textbf{optimising compiler}.

The optimising compiler operates on the level of a single function. It starts by doing a
depth-first search to discover all the basic blocks in the function. It then iterates over this
again producing Cranelift IR Format (CLIF) which is passed to the machinery of
\textbf{cranelift}. The CLIF code produced abstracts away the use of the stack allowing cranelift
to perform its register allocation. The function's code pointer is updated and any future calls
(including the originally triggering call) will use the optimised implementation.

\section{Development methodology}

\section{Instruction parsing}

\subsection{Types}

Maybe I should talk about the Instruction type that gets used everywhere

\subsection{Parser}

\section{The initial compiler}

There were two milestones in the project where the initial compiler was complete: after the
implementation of the initial compiler and after the integration with the optimising compiler. The
essential difference between them is the addition of a level of pointer indirection at function
calls to allow patching the address of a function after it has been optimised.

I will describe the first implementation in this section and describe the modifications made to
support dynamic recompilation in the subsequent section REF.

\subsection{Overview of design}

In order to ensure correctness, the initial compiler produces assembly with \emph{exactly the same
    semantics} as the interpreter does (with minor exceptions covered later). The exact same
operations
are performed in the same order as the bytecode running in the interpreter performs.

The only difference is rather than use bytecode, the operations are inlined into direct assembly.
Bytecode branches are translated to machine code branches. There are two reasons why it is
reasonable to expect this to be faster:

\begin{enumerate}
    \item This design integrates much better into the instruction pipelines, branch prediction and
          out-of-order execution modern CPUs are capable of
    \item Operands can be hard-coded into the machine code rather than requiring another memory
          lookup. For example, rather than performing generalised pointer addition when looking up
          a field
          the exact offset can be included in the addressing mode.
\end{enumerate}

\subsection{Main library used}

The project makes use of the \texttt{dynasm-rs} library. The design of this library allows for
writing assembly code snippets in the source code. The assembly is validated and mostly assembled
at compile time using Rust procedural macros and a much smaller runtime library deals with any
relocations and labels.

This library was a great success as it allowed me to operate at the level of the library without
being concerned about how the assembly maps to binary. The ahead-of-time aspects means there was
very little overhead at runtime - much less than alternative approaches like making an assembly
builder or printing assembly and forking out to an assembler would use.

\subsection{Mapping of the abstract machine}

The first compiler uses a fairly direct translation from OCaml bytecode to assembly.

The x86\_64 registers \texttt{r12-r15} (callee-saved in the System V calling convention) are used
to store the OCaml registers - however the system PC is used instead of the bytecode pointer.

\subsection{Implementation overview}

The compiler is triggered on the first time a `section' is loaded - for normal programs this is at
startup and for programs using the OCaml toplevel REPL this is after every statement is typed.

The compiler first emits a standard function header entrypoint which saves callee-saved registers
used by the compiler and aligns the C stack. A longjmp handler is set up for exceptions and then
for each bytecode instruction assembly with the same semantics is emitted.

During this process \texttt{dynasm-rs} dynamic labels are used to set up relocations: these
labels are defined before every bytecode instruction and can be referenced by any other
instruction. DynASM translates these at runtime into pc-relative jumps.

After all instructions are done, some shared code used by the instructions is emitted.
\texttt{dynasm-rs} then performs relocations and uses \texttt{mmap} to mark the region of code as
executable.

The overall signature of the assembly produced by this process is a single function taking no
arguments and returning an OCaml value. OCaml closure applications (function calls) do not produce
a stack frame on the C stack - the existing machinery using the OCaml stack is used instead.

The main code of the compiler itself is contained in a 2000 line file
(\texttt{src/rust/ocaml-jit-staticlib/src/compiler/emit\_code.rs}). Most of this is taken up by a
Rust large pattern match for each of the bytecode instructions. As a very simple example of what
the code looks like consider the implemenation of the \texttt{Add} instruction. It adds the value
at the top of the OCaml stack to the accumulator and stores the result in the accumulator. Note
that the OCaml integer format means a decrement is required. In the original assembler it is
implemented as so:

\inputminted{c}{snippets/add.c}

In the compiler the case is instead:

\inputminted{rust}{snippets/add.rs}

which has exactly the same semantics. Note \texttt{r\_accu} and \texttt{r\_sp}
are aliases for \texttt{r13} and \texttt{r15}.

I repeated this process for every bytecode instruction. More involved instructions call into
OCaml runtime or custom-written C functions - the state of the registers is pushed to the stack
and interpreted as a struct by the calling functions.

\subsection{Further details}

The above explanation is an over-simplification. Here is an non-exhaustive list of complicating
factors:

\begin{itemize}
    \item There is extensive optional support to support tracing instructions and events (described
          in section \ref{tracing})
    \item Callbacks from C to OCaml code require special handling
    \item The compiler returns a pointer to the first instruction to support OCaml's
          metaprogramming \texttt{ocaml\_reify\_bytecode} program used in the implementation of
          things like the
          toplevel \texttt{ocaml} program.
    \item Function application is a fairly involved process and there are checks to resize the
          stack and check for signals as well as the fundamental complexity of the push-enter
          model.
    \item Registers need to be saved and restored at safepoints to allow the garbage collector to
          use them
    \item Certain operations are involved enough that instead of inline the definition in
          hand-written assembly, I push the registers to the C stack and call a C primitive to
          implement the
          operation (taking the registers as a struct)
    \item The compiler stores a persistent data structure of the sections to allow mapping bytecode
          addresses to
          machine code addresses and allow for cleanup after a section is freed.
\end{itemize}

Some of these are covered later but for some I will only mention them here to note their existence
- the actual details are in the code.

\subsection{Implementation strategy}

Implementation followed an incremental and highly test-driven strategy over multiple weeks. The
initial focus was on building a system sophisticated enough to run a hello world program
implementing the bare minimum instructions to support this.

I then slowly expanded the complexity of programs using them to drive the implementation of new
instructions and the fixing of bugs in the previous programs.

As is mostly inevitable in a project of this complexity hand-writing assembly there were a
significant number of bugs. Traditional debugging methods like print debugging or using a debugger
could not be as easily applied. Some errors resulted in confusing segfaults without any clear
indication
of what went wrong (although by poking around in the memory with GDB I was able to fix them).

Despite this the actual implementation was remarkably efficient. This is mainly due to the trace
comparison tooling I developed at the very start of the project and continued to expand throughout
the project.

\subsection{Trace comparison} \label{tracing}

There is no formal specification for the OCaml interpeter. The semantics of the interpreter are
what
\texttt{interp.c} and other files in the runtime say they are. Given this I decided to build
tooling to test the behaviour of my JIT-compiled code directly against the behaviour of the
interpreter.

\subsubsection{Tracing}

In order to do this I added support for tracing after every instruction in both the existing
interpreter and the JIT-compiled code. The log entry contains the instruction executed, state of
all of the OCaml registers and the top 5 entries on the stack.

There are a few log formats that it can print - for this application I used \texttt{serde} to
serialize and deserialize the trace entries as JSON. The trace comparison program launches the
program: once with the interpreter and once using the JITed code

\subsubsection{Comparison}

A wrapper program (in the \texttt{ocaml-jit-tools} crate) runs a sepcified program with
tracing enabled twice simultaneously - one run uses the JIT and the other the
existing interpreter.

Then for every trace entry received it compares the two lines. If there is a difference between the
lines
it shows a diff and then exits.

As many of the values are pointers there is a risk of non-determinism making this comparison fail.
I used a small wrapper program I found caled \texttt{no-aslr} to disable ASLR. In order to ensure
that the Rust code doesn't cause them to become unaligned I ran the compiler regardless of whether
JITed code was enabled when tracing was enabled. These two things together worked well enough that
all of the memory addresses were aligned and deterministic. This is unlikely to be true in general
for all OS kernels and malloc implemenations but worked for me.

The only expected difference comes from the use of the machine PC rather than the bytecode PC -
instruction pointers like return addresses on the stack could differ. This required a special case
during the check.

I added a script to run about 10 test programs that together mostly covered the entire instruction
set. Running this frequently allowed me to test for regressions when making changes.

\subsection{Towards full correctness}

Once I was happy that I had implemented every instruction, I started using
the OCaml compiler's internal test suite. I discovered some subtle bugs and used it to add new test
programs and fix them by trace comparison. One test heavily used callbacks from C to OCaml and I
discovered my initial implementation was too slow.

I eventually managed to get nearly all tests in the test suite working - the only failures were
testing the backtrace support and the debugger.

After this I successfuly managed to bootstrap the compiler using the JIT which gave me a high level
of confidence in the accuracy of the JIT-compiled code.

\subsection{Next steps}

The next steps were to add the benchmark suite. More detail is given later in the evaluation
chapter but the summary is the results were good and validated that this approach could be
performant. I had some time left to work on significant extensions.

\subsubsection{Initial plan and SSA form}

I noticed from looking at the structure of the compiled code that a large source of inefficiency
was the reliance on the stack machine model. This made nearly every operation involve reading from
or pushing to memory and made little use of hardware registers. Values would be pushed to memory
only to be dropped by a \texttt{Pop} operation later and all function arguments needed to be placed
on the stack.

To futher investigate the feasibility of doing something more feasible I decided to produce a
system which would parse the bytecode into an SSA-type intermediate representation where the OCaml
stack or accumulator registers were not explicitly used. To test this idea in isolation I decided
to develop it as a dissasembler. The output of the dissasembler is GraphViz \texttt{dot} files for
each functions which are converted into hyperlinked \texttt{svg} files.

The final version of the optimised compiler did the same conversion from explicit stack to implicit
stack with SSA but performed it in a different way. The reasons for these differences are explained
later in section REF. However the code still appears in the final deliverable as part of the
`clever' dissasembler.

\subsubsection{Optimised compiler}

Having developed this SSA form I had two choices for how to proceed.

\begin{enumerate}
    \item write an x86\_64 compiler backend from scratch
    \item use a compiler framework to do the translation from an IR that is closer in semantics to
          the existing SSA
\end{enumerate}

With the limited time remaining and considering the scope of the project I decided that the first
option was infeasible. For the second, the typical choice to use is LLVM. LLVM is developed by
Apple and used to support many languages and compilers including Rust, Swift and Clang for C.
Although typically used for ahead-of-time compilation it has support for use in JIT compilation. It
additionally contains some support for garbage collection including a model that can be used for
safepoints.

I initially decided to go with LLVM but as the implementation started I ran into some limitations.

\begin{itemize}
    \item LLVM is very large and bloated - compiling it with multiple jobs caused my machine to run
          out of memory and linking it in massively bloated the binary size of \texttt{ocamlrun}
    \item The safepoint GC support while somewhat mature is messy and would require writing
          patching the C++ source of LLVM to add new options as well as diving very deep into
          internal data
          structures
    \item LLVM has about 3 different JIT interfaces (\texttt{MCJit}, \texttt{orc}, \texttt{orc2}),
          all of which had different limitations when used in a project such as mine
    \item Although there are some very good Rust bindings, not all features of the C++ api are
          exposed. This is especially true with the garbage collector support.
    \item LLVM is somewhat heavyweight and slow to compile which makes it inherently less well
          suited for use in this project
\end{itemize}

It was at this point that I discovered the \texttt{cranelift} project which seemed in many ways to
be exactly what I needed.

\begin{itemize}
    \item It is written in Rust which means the API is fairly idiomatic when using it in Rust
    \item It is designed primarily for JITs and focuses heavily on compilation performance
    \item The project is actively developed and I could communicate with the developers when I had
          questions who were very responsive.
    \item Although the support for garbage collection is not as extensible, with the help of the
          developers I was able to come up with a model that is a good fit for the OCaml garbage
          collector's
          requirements.
    \item When I encountered any bugs or missing features I could easily submit and get merged
          patches to the project which uses a familiar Github workflow rathter than the LLVM
          project's legacy
          systems.
\end{itemize}

There were still some missing features in Cranelift which are described later but for the most
part it was a good fit for my needs and was in a large part responsible for my being able to
finish this project in time.

\section{Modifications to support dynamic recompilation}

\subsection{Closure metadata table}

OCaml closures consist of a heap-allocated block containing a code pointer and the closure env
variables. In the original interpreter this code pointer pointed at a bytecode instruction. In this
second this was replaced with a pointer to the machine code.

However, different closures can use the same code pionter. The optimsed compiler optimises the code
in isolation rather than the specific instance in the closure. We also need to keep a count of the
number of times a section of code is executed to know when to branch into the compiler.

To do these tasks, I extended the initial compiler to have a pass to discover all of the closures
referenced in the program. For each of these I allocate a 32 byte entry in a table and modified the
code pointers in closusres to point to these values.

The format of this is INCLUDE THE FORMAT HERE.

\subsection{Transforming partially to eval-apply}

As described in section REFTODO, the OCaml interpreter uses a push-enter model for function calls.
Projects like Cranelift and LLVM tend to use a calling model which is closer to that of C to
support the platform ABI. This is also what the CPUs themseleves are optimsed to do.

For this reason I decided to convert from the push-enter model (callee deals with arity mismatch)
to an eval-apply model (caller deals with arity mismatch).

There is already some shared tasks performed on every apply - checking for stack resizing and
checking signals. I modified every apply in such a way that dealing with

For simplicity, I only decided to support the optimisation of functions taking 5 or fewer arguments
which is the vast majority of them. Arguments are passed according to the C calling convention with
the closure environment always passed as the first argument.

\textbf{MENTION needing to support optimised calls from cranelifted to cranelifted}

\section{Optimised compiler}

\subsection{Cranelift design}

Cranelift is a THING similar to LLVM.

This means that a goal is to have a mostly target-independent format which can be shared between
the backends.

Cranelift IR is in SSA. In order to make this easier to use cranelift includes an online IR builder
based on the work of LINKPAPERFORCRANELIFTSHIT.

Unlike LLVM, cranelift uses block parameters rather than Phi nodes. Link to the paper. In practice,
this is a slightly nicer model to work with.

Cranelift is a typed IR with integer types, functions and crucially for our needs reference types.
These were originally created to support webaseembly reference types whose garbage collection uses
reference counting but it's fully precise (DEFINE) nature means it can be used to fit our needs.

\subsection{The optimised compiler}

The steps taken are

\begin{enumerate}
    \item Convert the instructions into basic blocks and analyse stack starts
    \item Translate each basic block into Cranelift instructions using cranelift variables to deal
          with variable positions
    \item Run cranelift on the source
    \item Store stack maps emitted by cranelift in the hashmap for garbage collection
\end{enumerate}

\subsection{Basic block conversion and stack starts}

The compiler starts by parsing the instructions into basic block instructions. The algorithm is
essentially a depth first search where the stack pointer is updated.

Give pseudo-code for the search

Once this is done we are ready to convert the basic blocks to the form that cranelift expects.

\subsection{Conversion to Cranelift IR}

The structure of this is remarkably similar to the interpreter, and the original compiler. The only
difference is rather than pushing and popping.

Cranelift works with the concept of an IR builder which is based around the core Value type. For
example \texttt{2 + 3} might naively be translated into this sequence of calls

\begin{minted}{rust}
let av = self.builder.ins().iconst(types::I64, 2); // av : Value
let bv = self.builder.ins().iconst(types::I64, 3); // bv : value
let result = self.builder.ins().iadd(av, bv);      // result: Value
\end{minted}

\section{Trace comparison for the Cranelift compiler}

In order to have any hope of creating a correct mapping for cranelift code to

Talk about apply tracing using the existing compiler as the gold standard

\subsection{Garbage collection support}

Talk about R64 and I64 types, conversion between them and what Cranelift does RE spilling and stack
slots

\subsubsection{Unwinding with \texttt{libunwind}}

Talk about problems with using libunwind and generic DWARF debug info

\subsubsection{\texttt{rbp} unwinding}

Compiling with no emit frame pointer made this much easier

\subsubsection{Final design}

\subsection{Bigger picture}

test

\section{Disassembly tools}

\subsection{SSA dissassembler}

\subsubsection{Unifying stack variables}

\subsubsection{Parsing debug info}

\section{Overview of repository}

\dirtree{%
    .1 /.
    .2 benchmarks.
    .3 sandmark\DTcomment{fork of the Sandmark benchmark suite to support bytecode}.
    .3 analysis\DTcomment{Jupyter notebooks analysing benchmark results}.
    .2 docs\DTcomment{{\LaTeX} source of proposal, report and this document}.
    .2 scripts\DTcomment{scripts to run tests and graph basic blocks}.
    .3 run\_tests.sh\DTcomment{runs the entire suite of benchmark tests}.
    .2 src.
    .3 rust.
    .4 ocaml-jit-shared\DTcomment{shared library between the other two crates}.
    .4 ocaml-jit-staticlib\DTcomment{crate linked in to OCaml runtime}.
    .4 ocaml-jit-tools\DTcomment{standalone tools used for testing}.
    .3 ocaml\DTcomment{contains a fork of the entire OCaml compiler}.
    .4 runtime\DTcomment{where most modifications to the OCaml compiler happened}.
    .5 jit\_support.c\DTcomment{contains C primitives used by the compiled code}.
    .3 vendor\DTcomment{contains forked Rust dependencies}.
    .2 test-programs\DTcomment{OCaml source for the test programs used by the scripts}.
    .2 no-aslr\DTcomment{A simple wrapper to run a program without ASLR}.
}

\subsection{ocaml-jit-shared crate}

\dirtree{%
    .1 src/rust/ocaml-jit-shared.
    .2 Cargo.toml\DTcomment{specifies dependencies}.
    .2 src.
    .3 basic\_blocks\DTcomment{contains types and algorithm for converting an instruction stream to
        basic blocks}.
    .3 cranelift\_compiler.
    .4 mod.rs\DTcomment{Contains the bulk of the implementation of the optimised compiler}.
    .4 test\_cases\DTcomment{Contains many expect-test cases}.
    .3 instructions.
    .4 parse.rs\DTcomment{contains the parser for OCaml bytecode}.
    .4 types.rs\DTcomment{defines the core instruction types used everywhere}.
}

\subsection{ocaml-jit-staticlib crate}

\dirtree{%
    .1 src/rust/ocaml-jit-staticlib.
    .2 Cargo.toml\DTcomment{specifies dependencies}.
    .2 src.
    .3 caml\DTcomment{contains Rust wrappers for OCaml headers}.
    .3 compiler.
    .4 c\_primitives.rs\DTcomment{imports C primitives from the OCaml runtime}.
    .4 emit\_code.rs\DTcomment{contains the bulk of the implementation of the initial compiler}.
    .4 rust\_primitives.rs\DTcomment{contains primitives written in Rust called by JITed code}.
    .4 saved\_data.rs\DTcomment{defines the persistent data structures the compiler adds}.
    .3 c\_entrypoints.rs\DTcomment{glue for C to Rust FFI}.
    .3 configuration.rs\DTcomment{defines the env-var options the JIT has}.
    .3 lib.rs\DTcomment{contains the entrypoints into the Rust code}.
}
